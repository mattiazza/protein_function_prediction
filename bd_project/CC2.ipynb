{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CC (Elia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "output_folder = Path('../output')\n",
    "X = np.load(output_folder / 'x_remb_CC.npy')\n",
    "y_df = pd.read_csv(output_folder / 'y_df_CC.csv')\n",
    "# X_sparse = csr_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "y_df['GO_term'] = y_df['GO_term'].apply(lambda x: list(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess target (MultiLabelBinarizer for multi-label classification)\n",
    "mlb = MultiLabelBinarizer()  # Enable sparse output\n",
    "y = mlb.fit_transform(y_df['GO_term'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sparse shape: (84638, 274)\n",
      "y shape: (84638, 678)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_sparse shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y.astype(np.float32)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 6.0130\n",
      "Epoch [2/50], Loss: 2.9623\n",
      "Epoch [3/50], Loss: 2.9182\n",
      "Epoch [4/50], Loss: 2.8930\n",
      "Epoch [5/50], Loss: 2.8750\n",
      "Epoch [6/50], Loss: 2.8602\n",
      "Epoch [7/50], Loss: 2.8497\n",
      "Epoch [8/50], Loss: 2.8394\n",
      "Epoch [9/50], Loss: 2.8318\n",
      "Epoch [10/50], Loss: 2.8260\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m train_model(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size, epochs)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[0;32m     70\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[1;32mIn[48], line 59\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, y_train, optimizer, criterion, batch_size, epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     62\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "# Convert data to PyTorch tensors (using full embeddings)\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32).to(device)  # Full embeddings: shape (44558, 1024)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)  # Labels: shape (44558, num_labels)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32).to(device)    # Full embeddings: shape (test_size, 1024)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)  # Labels: shape (test_size, num_labels)\n",
    "\n",
    "\n",
    "# Define the Neural Network\n",
    "class MultilabelNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MultilabelNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Dropout for regularization\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_sizes[1], output_size),\n",
    "            nn.Sigmoid()  # For multilabel classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train_tensor.shape[1]  # 300 features\n",
    "hidden_sizes = [1024, 512, 256]            # Hidden layer sizes\n",
    "output_size = y_train.shape[1]    # Number of labels\n",
    "\n",
    "# Initialize the model\n",
    "model = MultilabelNN(input_size, hidden_sizes, output_size).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 1024\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, X_train, y_train, optimizer, criterion, batch_size, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size, epochs)\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_binary = (y_pred.cpu().numpy() > 0.5).astype(int)\n",
    "\n",
    "# Classification report\n",
    "target_names = [str(cls) for cls in mlb.classes_]\n",
    "print(\"\\nNeural Network Results:\")\n",
    "print(classification_report(y_test, y_pred_binary, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Epoch 1/30\n",
      "Epoch Time: 5.9s, Estimated Remaining Time: 171s\n",
      "Epoch 11/30\n",
      "Epoch Time: 4.4s, Estimated Remaining Time: 84s\n",
      "Epoch 21/30\n",
      "Epoch Time: 4.4s, Estimated Remaining Time: 40s\n",
      "Epoch 30/30\n",
      "Epoch Time: 4.4s\n",
      "Fold 1: Macro F1: 0.010, Micro F1: 0.563, Weighted F1: 0.392, Sample F1: 0.579\n",
      "Total time for fold 1: 149.43s\n",
      "\n",
      "FOLD 2\n",
      "Epoch 1/30\n",
      "Epoch Time: 5.2s, Estimated Remaining Time: 151s\n",
      "Epoch 11/30\n",
      "Epoch Time: 5.5s, Estimated Remaining Time: 104s\n",
      "Epoch 21/30\n",
      "Epoch Time: 6.0s, Estimated Remaining Time: 54s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 125\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, precision_recall_summary\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Run the model\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m results, precision_recall_summary \u001b[38;5;241m=\u001b[39m train_and_evaluate_model(X_new, y_new)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Summarize performance\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[19], line 70\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(X, y, n_splits, n_repeats, epochs, batch_size, threshold)\u001b[0m\n\u001b[0;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Calculate elapsed time for the current epoch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, f1_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import numpy as np\n",
    "import time  # Import time module\n",
    "\n",
    "\n",
    "# Define the PyTorch model with larger layers\n",
    "class MultiLabelNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(MultiLabelNN, self).__init__()\n",
    "        self.model=nn.Sequential(nn.Linear(n_inputs, 2048),  # Very large hidden layer\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.3),\n",
    "                                 nn.Linear(2048, 1024),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.3),\n",
    "                                 nn.Linear(1024, n_outputs),\n",
    "                                 nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train and evaluate the model\n",
    "def train_and_evaluate_model(X, y, n_splits=3, n_repeats=1, epochs=30, batch_size=32, threshold=0.5):\n",
    "    results = {\"macro_f1\": [], \"micro_f1\": [], \"weighted_f1\": [], \"sample_f1\": []}\n",
    "    precision_recall_summary = []\n",
    "\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X), 1):\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "        # Initialize model, loss, and optimizer\n",
    "        model = MultiLabelNN(n_inputs, n_outputs)\n",
    "        \n",
    "        criterion = nn.BCELoss()  # Binary cross-entropy loss for multi-label\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        #optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "        # Record start time for the entire fold\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"FOLD {fold_idx}\")\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()  # Start time for the current epoch\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_train_tensor)\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate elapsed time for the current epoch\n",
    "            epoch_elapsed_time = time.time() - epoch_start_time\n",
    "            avg_epoch_time = epoch_elapsed_time\n",
    "\n",
    "            # Estimate remaining time\n",
    "            remaining_epochs = epochs - (epoch + 1)\n",
    "            remaining_time = remaining_epochs * avg_epoch_time\n",
    "\n",
    "            # Print progress\n",
    "            if (epoch % 10 == 0):\n",
    "                print(f'Epoch {epoch + 1}/{epochs}\\n'\n",
    "                    f'Epoch Time: {epoch_elapsed_time:.1f}s, '\n",
    "                    f'Estimated Remaining Time: {remaining_time:.0f}s')\n",
    "            elif (epoch == epochs - 1):\n",
    "                print(f'Epoch {epochs}/{epochs}\\n'\n",
    "                    f'Epoch Time: {epoch_elapsed_time:.1f}s')\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            yhat = model(X_test_tensor).numpy()\n",
    "            yhat_binary = (yhat > threshold).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = precision_recall_fscore_support(y_test, yhat_binary, average=None, zero_division=0)\n",
    "            precision, recall, f1_scores, _ = metrics\n",
    "            macro_f1 = f1_score(y_test, yhat_binary, average='macro')\n",
    "            micro_f1 = f1_score(y_test, yhat_binary, average='micro')\n",
    "            weighted_f1 = f1_score(y_test, yhat_binary, average='weighted')\n",
    "            sample_f1 = f1_score(y_test, yhat_binary, average='samples')\n",
    "\n",
    "            # Store results\n",
    "            results[\"macro_f1\"].append(macro_f1)\n",
    "            results[\"micro_f1\"].append(micro_f1)\n",
    "            results[\"weighted_f1\"].append(weighted_f1)\n",
    "            results[\"sample_f1\"].append(sample_f1)\n",
    "\n",
    "            precision_recall_summary.append(\n",
    "                {\"precision_avg\": np.mean(precision), \"recall_avg\": np.mean(recall)}\n",
    "            )\n",
    "\n",
    "            print(f'Fold {fold_idx}: Macro F1: {macro_f1:.3f}, Micro F1: {micro_f1:.3f}, '\n",
    "                  f'Weighted F1: {weighted_f1:.3f}, Sample F1: {sample_f1:.3f}')\n",
    "\n",
    "        # Calculate total time for the fold\n",
    "        total_fold_time = time.time() - start_time\n",
    "        print(f'Total time for fold {fold_idx}: {total_fold_time:.2f}s\\n')\n",
    "\n",
    "    return results, precision_recall_summary\n",
    "\n",
    "\n",
    "# Run the model\n",
    "results, precision_recall_summary = train_and_evaluate_model(X_new, y_new)\n",
    "\n",
    "\n",
    "# Summarize performance\n",
    "for key in results:\n",
    "    print(f'{key}: Mean={np.mean(results[key]):.3f}, Std={np.std(results[key]):.3f}')\n",
    "\n",
    "print('Average Precision/Recall per fold:')\n",
    "for i, summary in enumerate(precision_recall_summary, 1):\n",
    "    print(f'Fold {i}: Precision: {summary[\"precision_avg\"]:.3f}, Recall: {summary[\"recall_avg\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noccapito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert sparse matrices to dense arrays\u001b[39;00m\n\u001b[0;32m     10\u001b[0m X_array \u001b[38;5;241m=\u001b[39m X_sparse\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m---> 11\u001b[0m Y_array \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     13\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_array, Y_array, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X_array = X_sparse.toarray()\n",
    "Y_array = y.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_array, Y_array, test_size=0.2)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m multi_clf \u001b[38;5;241m=\u001b[39m MultiOutputClassifier(rf_clf, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m multi_clf\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m multi_clf\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\multioutput.py:543\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[1;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, Y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    275\u001b[0m     delayed(_fit_estimator)(\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y[:, i], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit\n\u001b[0;32m    277\u001b[0m     )\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Handle class imbalance: Compute class weights for multilabel classification\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.flatten())\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Create a RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=10,       # Number of trees\n",
    "    max_depth=4,          # Maximum depth of trees\n",
    "    n_jobs=-1,             # Use all CPU cores\n",
    "    random_state=42,\n",
    "    verbose=5,\n",
    "    class_weight=class_weight_dict  # Use the computed class weights\n",
    ")\n",
    "\n",
    "# Create the MultiOutputClassifier\n",
    "multi_clf = MultiOutputClassifier(rf_clf, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "multi_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = multi_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  GO:0000118       0.00      0.00      0.00        44\n",
      "  GO:0000123       0.00      0.00      0.00        67\n",
      "  GO:0000124       0.00      0.00      0.00        23\n",
      "  GO:0000131       0.00      0.00      0.00        15\n",
      "  GO:0000137       0.00      0.00      0.00        20\n",
      "  GO:0000138       0.00      0.00      0.00        25\n",
      "  GO:0000139       0.00      0.00      0.00       133\n",
      "  GO:0000145       0.00      0.00      0.00         8\n",
      "  GO:0000151       0.00      0.00      0.00       131\n",
      "  GO:0000152       0.00      0.00      0.00        30\n",
      "  GO:0000176       0.00      0.00      0.00         5\n",
      "  GO:0000178       0.00      0.00      0.00        11\n",
      "  GO:0000228       0.00      0.00      0.00       159\n",
      "  GO:0000235       0.00      0.00      0.00        15\n",
      "  GO:0000307       0.00      0.00      0.00        26\n",
      "  GO:0000313       0.00      0.00      0.00        85\n",
      "  GO:0000314       0.00      0.00      0.00        31\n",
      "  GO:0000315       0.00      0.00      0.00        51\n",
      "  GO:0000322       0.00      0.00      0.00       128\n",
      "  GO:0000323       0.02      0.66      0.04       352\n",
      "  GO:0000324       0.00      0.00      0.00       126\n",
      "  GO:0000325       0.00      0.00      0.00       154\n",
      "  GO:0000329       0.00      0.00      0.00        64\n",
      "  GO:0000407       0.00      0.00      0.00        17\n",
      "  GO:0000421       0.00      0.00      0.00        15\n",
      "  GO:0000428       0.00      0.00      0.00       102\n",
      "  GO:0000502       0.00      0.00      0.00        69\n",
      "  GO:0000775       0.00      0.00      0.00       153\n",
      "  GO:0000776       0.00      0.00      0.00       101\n",
      "  GO:0000779       0.00      0.00      0.00       109\n",
      "  GO:0000781       0.00      0.00      0.00        69\n",
      "  GO:0000785       0.03      0.94      0.05       466\n",
      "  GO:0000786       0.00      0.00      0.00        17\n",
      "  GO:0000791       0.00      0.00      0.00        43\n",
      "  GO:0000792       0.00      0.00      0.00        63\n",
      "  GO:0000793       0.00      0.00      0.00       174\n",
      "  GO:0000794       0.00      0.00      0.00        50\n",
      "  GO:0000795       0.00      0.00      0.00        23\n",
      "  GO:0000803       0.00      0.00      0.00        16\n",
      "  GO:0000922       0.00      0.00      0.00        55\n",
      "  GO:0000930       0.00      0.00      0.00         5\n",
      "  GO:0000932       0.00      0.00      0.00        42\n",
      "  GO:0000935       0.00      0.00      0.00        11\n",
      "  GO:0000940       0.00      0.00      0.00        23\n",
      "  GO:0000974       0.00      0.00      0.00        12\n",
      "  GO:0001411       0.00      0.00      0.00        21\n",
      "  GO:0001533       0.00      0.00      0.00        18\n",
      "  GO:0001650       0.00      0.00      0.00        67\n",
      "  GO:0001669       0.00      0.00      0.00        43\n",
      "  GO:0001673       0.00      0.00      0.00        20\n",
      "  GO:0001674       0.00      0.00      0.00        11\n",
      "  GO:0001725       0.00      0.00      0.00        23\n",
      "  GO:0001726       0.00      0.00      0.00        39\n",
      "  GO:0001750       0.00      0.00      0.00        40\n",
      "  GO:0001772       0.00      0.00      0.00        15\n",
      "  GO:0001891       0.00      0.00      0.00        17\n",
      "  GO:0001917       0.00      0.00      0.00        24\n",
      "  GO:0005575       1.00      1.00      1.00     16928\n",
      "  GO:0005576       0.08      1.00      0.14      1317\n",
      "  GO:0005581       0.00      0.00      0.00        15\n",
      "  GO:0005604       0.00      0.00      0.00        38\n",
      "  GO:0005615       0.06      1.00      0.11       982\n",
      "  GO:0005618       0.02      0.12      0.03       307\n",
      "  GO:0005622       0.80      1.00      0.89     13489\n",
      "  GO:0005628       0.00      0.00      0.00        17\n",
      "  GO:0005634       0.32      1.00      0.49      5431\n",
      "  GO:0005635       0.01      0.03      0.02       263\n",
      "  GO:0005637       0.00      0.00      0.00        15\n",
      "  GO:0005640       0.00      0.00      0.00        10\n",
      "  GO:0005643       0.00      0.00      0.00        57\n",
      "  GO:0005654       0.13      1.00      0.23      2166\n",
      "  GO:0005657       0.00      0.00      0.00        45\n",
      "  GO:0005665       0.00      0.00      0.00        18\n",
      "  GO:0005667       0.00      0.00      0.00       212\n",
      "  GO:0005669       0.00      0.00      0.00        13\n",
      "  GO:0005680       0.00      0.00      0.00        14\n",
      "  GO:0005681       0.00      0.00      0.00       115\n",
      "  GO:0005684       0.00      0.00      0.00        29\n",
      "  GO:0005685       0.00      0.00      0.00        12\n",
      "  GO:0005686       0.00      0.00      0.00         7\n",
      "  GO:0005694       0.05      0.99      0.09       842\n",
      "  GO:0005700       0.00      0.00      0.00        64\n",
      "  GO:0005703       0.00      0.00      0.00        11\n",
      "  GO:0005705       0.00      0.00      0.00        21\n",
      "  GO:0005721       0.00      0.00      0.00        24\n",
      "  GO:0005730       0.03      0.99      0.07       579\n",
      "  GO:0005732       0.00      0.00      0.00         8\n",
      "  GO:0005737       0.60      1.00      0.75     10152\n",
      "  GO:0005739       0.10      1.00      0.19      1742\n",
      "  GO:0005740       0.02      0.86      0.05       405\n",
      "  GO:0005741       0.00      0.00      0.00        99\n",
      "  GO:0005742       0.00      0.00      0.00        11\n",
      "  GO:0005743       0.02      0.02      0.02       256\n",
      "  GO:0005746       0.00      0.00      0.00        62\n",
      "  GO:0005747       0.00      0.00      0.00        35\n",
      "  GO:0005750       0.00      0.00      0.00        11\n",
      "  GO:0005751       0.00      0.00      0.00        13\n",
      "  GO:0005753       0.00      0.00      0.00        24\n",
      "  GO:0005758       0.00      0.00      0.00        42\n",
      "  GO:0005759       0.04      0.00      0.01       242\n",
      "  GO:0005761       0.00      0.00      0.00        84\n",
      "  GO:0005762       0.00      0.00      0.00        51\n",
      "  GO:0005763       0.00      0.00      0.00        31\n",
      "  GO:0005764       0.00      0.00      0.00       225\n",
      "  GO:0005765       0.00      0.00      0.00        98\n",
      "  GO:0005766       0.00      0.00      0.00        23\n",
      "  GO:0005767       0.00      0.00      0.00        17\n",
      "  GO:0005768       0.02      0.89      0.05       413\n",
      "  GO:0005769       0.00      0.00      0.00        99\n",
      "  GO:0005770       0.00      0.00      0.00        84\n",
      "  GO:0005771       0.00      0.00      0.00        24\n",
      "  GO:0005773       0.04      0.99      0.07       597\n",
      "  GO:0005774       0.00      0.00      0.00       216\n",
      "  GO:0005775       0.00      0.00      0.00        37\n",
      "  GO:0005776       0.00      0.00      0.00        44\n",
      "  GO:0005777       0.00      0.00      0.00       209\n",
      "  GO:0005778       0.00      0.00      0.00        40\n",
      "  GO:0005782       0.00      0.00      0.00        23\n",
      "  GO:0005783       0.06      1.00      0.12      1043\n",
      "  GO:0005788       0.00      0.00      0.00        81\n",
      "  GO:0005789       0.02      0.37      0.04       297\n",
      "  GO:0005790       0.00      0.00      0.00        14\n",
      "  GO:0005791       0.00      0.00      0.00        12\n",
      "  GO:0005793       0.00      0.00      0.00        20\n",
      "  GO:0005794       0.05      1.00      0.09       838\n",
      "  GO:0005795       0.00      0.00      0.00        83\n",
      "  GO:0005796       0.00      0.00      0.00        34\n",
      "  GO:0005797       0.00      0.00      0.00        22\n",
      "  GO:0005798       0.00      0.00      0.00        52\n",
      "  GO:0005801       0.00      0.00      0.00        25\n",
      "  GO:0005802       0.00      0.00      0.00       130\n",
      "  GO:0005811       0.00      0.00      0.00        74\n",
      "  GO:0005813       0.02      0.01      0.01       259\n",
      "  GO:0005814       0.00      0.00      0.00        43\n",
      "  GO:0005815       0.02      0.90      0.05       406\n",
      "  GO:0005816       0.00      0.00      0.00        52\n",
      "  GO:0005818       0.00      0.00      0.00        22\n",
      "  GO:0005819       0.08      0.00      0.01       203\n",
      "  GO:0005826       0.00      0.00      0.00        20\n",
      "  GO:0005829       0.21      1.00      0.34      3485\n",
      "  GO:0005832       0.00      0.00      0.00        10\n",
      "  GO:0005834       0.00      0.00      0.00        18\n",
      "  GO:0005838       0.00      0.00      0.00        19\n",
      "  GO:0005839       0.00      0.00      0.00        23\n",
      "  GO:0005840       0.02      0.05      0.03       284\n",
      "  GO:0005844       0.00      0.00      0.00        53\n",
      "  GO:0005847       0.00      0.00      0.00        14\n",
      "  GO:0005849       0.00      0.00      0.00        16\n",
      "  GO:0005852       0.00      0.00      0.00        12\n",
      "  GO:0005856       0.07      1.00      0.12      1100\n",
      "  GO:0005865       0.00      0.00      0.00        16\n",
      "  GO:0005871       0.00      0.00      0.00        15\n",
      "  GO:0005874       0.00      0.00      0.00       152\n",
      "  GO:0005875       0.00      0.00      0.00        67\n",
      "  GO:0005876       0.00      0.00      0.00        51\n",
      "  GO:0005879       0.00      0.00      0.00        19\n",
      "  GO:0005881       0.00      0.00      0.00        67\n",
      "  GO:0005882       0.00      0.00      0.00        25\n",
      "  GO:0005884       0.00      0.00      0.00        40\n",
      "  GO:0005885       0.00      0.00      0.00        12\n",
      "  GO:0005886       0.19      1.00      0.31      3153\n",
      "  GO:0005891       0.00      0.00      0.00        13\n",
      "  GO:0005892       0.00      0.00      0.00        12\n",
      "  GO:0005901       0.00      0.00      0.00        34\n",
      "  GO:0005902       0.00      0.00      0.00        31\n",
      "  GO:0005903       0.00      0.00      0.00        48\n",
      "  GO:0005905       0.00      0.00      0.00        24\n",
      "  GO:0005911       0.02      0.73      0.05       396\n",
      "  GO:0005912       0.00      0.00      0.00        65\n",
      "  GO:0005918       0.00      0.00      0.00        13\n",
      "  GO:0005921       0.00      0.00      0.00        18\n",
      "  GO:0005923       0.00      0.00      0.00        31\n",
      "  GO:0005925       0.00      0.00      0.00       116\n",
      "  GO:0005927       0.00      0.00      0.00         5\n",
      "  GO:0005929       0.03      0.98      0.06       539\n",
      "  GO:0005930       0.00      0.00      0.00        98\n",
      "  GO:0005933       0.00      0.00      0.00        55\n",
      "  GO:0005934       0.00      0.00      0.00        22\n",
      "  GO:0005935       0.00      0.00      0.00        41\n",
      "  GO:0005937       0.00      0.00      0.00        29\n",
      "  GO:0005938       0.01      0.02      0.02       278\n",
      "  GO:0005942       0.00      0.00      0.00        15\n",
      "  GO:0008021       0.00      0.00      0.00        88\n",
      "  GO:0008023       0.00      0.00      0.00        34\n",
      "  GO:0008076       0.00      0.00      0.00        25\n",
      "  GO:0008180       0.00      0.00      0.00        19\n",
      "  GO:0008287       0.00      0.00      0.00        24\n",
      "  GO:0008305       0.00      0.00      0.00        12\n",
      "  GO:0008328       0.00      0.00      0.00        14\n",
      "  GO:0009274       0.00      0.00      0.00       165\n",
      "  GO:0009277       0.00      0.00      0.00        58\n",
      "  GO:0009279       0.00      0.00      0.00        38\n",
      "  GO:0009295       0.00      0.00      0.00        43\n",
      "  GO:0009505       0.00      0.00      0.00        75\n",
      "  GO:0009506       0.00      0.00      0.00       172\n",
      "  GO:0009507       0.02      0.92      0.05       418\n",
      "  GO:0009524       0.00      0.00      0.00        15\n",
      "  GO:0009526       0.00      0.00      0.00       160\n",
      "  GO:0009528       0.00      0.00      0.00        14\n",
      "  GO:0009532       0.00      0.00      0.00       121\n",
      "  GO:0009534       0.00      0.00      0.00        92\n",
      "  GO:0009535       0.00      0.00      0.00        70\n",
      "  GO:0009536       0.04      0.99      0.08       663\n",
      "  GO:0009570       0.00      0.00      0.00       121\n",
      "  GO:0009579       0.00      0.00      0.00       117\n",
      "  GO:0009705       0.00      0.00      0.00        23\n",
      "  GO:0009706       0.00      0.00      0.00        12\n",
      "  GO:0009897       0.00      0.00      0.00       116\n",
      "  GO:0009898       0.00      0.00      0.00        77\n",
      "  GO:0009925       0.00      0.00      0.00       127\n",
      "  GO:0009941       0.00      0.00      0.00       109\n",
      "  GO:0009986       0.02      0.22      0.03       285\n",
      "  GO:0010008       0.00      0.00      0.00       103\n",
      "  GO:0010287       0.00      0.00      0.00         7\n",
      "  GO:0010369       0.00      0.00      0.00         9\n",
      "  GO:0010494       0.00      0.00      0.00        53\n",
      "  GO:0012505       0.15      1.00      0.26      2473\n",
      "  GO:0012506       0.02      0.27      0.03       309\n",
      "  GO:0012507       0.00      0.00      0.00        10\n",
      "  GO:0014069       0.00      0.00      0.00        89\n",
      "  GO:0014704       0.00      0.00      0.00        19\n",
      "  GO:0015030       0.00      0.00      0.00        11\n",
      "  GO:0015629       0.01      0.00      0.01       239\n",
      "  GO:0015630       0.04      0.99      0.08       687\n",
      "  GO:0015934       0.00      0.00      0.00       143\n",
      "  GO:0015935       0.00      0.00      0.00       106\n",
      "  GO:0016010       0.00      0.00      0.00         9\n",
      "  GO:0016020       0.28      1.00      0.43      4692\n",
      "  GO:0016028       0.00      0.00      0.00        15\n",
      "  GO:0016234       0.00      0.00      0.00        28\n",
      "  GO:0016235       0.00      0.00      0.00        11\n",
      "  GO:0016323       0.00      0.00      0.00       111\n",
      "  GO:0016324       0.00      0.00      0.00       168\n",
      "  GO:0016327       0.00      0.00      0.00        10\n",
      "  GO:0016328       0.00      0.00      0.00        28\n",
      "  GO:0016363       0.00      0.00      0.00        21\n",
      "  GO:0016459       0.00      0.00      0.00        16\n",
      "  GO:0016469       0.00      0.00      0.00        48\n",
      "  GO:0016471       0.00      0.00      0.00        12\n",
      "  GO:0016514       0.00      0.00      0.00        18\n",
      "  GO:0016528       0.00      0.00      0.00        30\n",
      "  GO:0016529       0.00      0.00      0.00        24\n",
      "  GO:0016581       0.00      0.00      0.00        10\n",
      "  GO:0016591       0.00      0.00      0.00        64\n",
      "  GO:0016592       0.00      0.00      0.00        27\n",
      "  GO:0016604       0.02      0.36      0.04       313\n",
      "  GO:0016605       0.00      0.00      0.00        21\n",
      "  GO:0016607       0.00      0.00      0.00       128\n",
      "  GO:0017053       0.00      0.00      0.00        30\n",
      "  GO:0018995       0.00      0.00      0.00       161\n",
      "  GO:0019005       0.00      0.00      0.00        27\n",
      "  GO:0019028       0.00      0.00      0.00        16\n",
      "  GO:0019866       0.02      0.10      0.03       286\n",
      "  GO:0019867       0.00      0.00      0.00       224\n",
      "  GO:0019897       0.00      0.00      0.00        46\n",
      "  GO:0019898       0.00      0.00      0.00        83\n",
      "  GO:0020002       0.00      0.00      0.00        32\n",
      "  GO:0020003       0.00      0.00      0.00        19\n",
      "  GO:0020007       0.00      0.00      0.00         8\n",
      "  GO:0020011       0.00      0.00      0.00        19\n",
      "  GO:0020015       0.00      0.00      0.00        38\n",
      "  GO:0020016       0.00      0.00      0.00        17\n",
      "  GO:0020023       0.00      0.00      0.00        60\n",
      "  GO:0020036       0.00      0.00      0.00        10\n",
      "  GO:0022624       0.00      0.00      0.00        24\n",
      "  GO:0022625       0.00      0.00      0.00        90\n",
      "  GO:0022626       0.00      0.00      0.00       182\n",
      "  GO:0022627       0.00      0.00      0.00        74\n",
      "  GO:0030014       0.00      0.00      0.00        11\n",
      "  GO:0030016       0.00      0.00      0.00       121\n",
      "  GO:0030017       0.00      0.00      0.00       100\n",
      "  GO:0030018       0.00      0.00      0.00        43\n",
      "  GO:0030027       0.00      0.00      0.00        45\n",
      "  GO:0030054       0.07      1.00      0.13      1158\n",
      "  GO:0030055       0.00      0.00      0.00       129\n",
      "  GO:0030117       0.00      0.00      0.00        44\n",
      "  GO:0030118       0.00      0.00      0.00        23\n",
      "  GO:0030119       0.00      0.00      0.00        19\n",
      "  GO:0030120       0.00      0.00      0.00        34\n",
      "  GO:0030125       0.00      0.00      0.00        18\n",
      "  GO:0030131       0.00      0.00      0.00        15\n",
      "  GO:0030133       0.00      0.00      0.00       151\n",
      "  GO:0030134       0.00      0.00      0.00        22\n",
      "  GO:0030135       0.00      0.00      0.00       117\n",
      "  GO:0030136       0.00      0.00      0.00        73\n",
      "  GO:0030137       0.00      0.00      0.00        23\n",
      "  GO:0030139       0.00      0.00      0.00       167\n",
      "  GO:0030140       0.00      0.00      0.00        21\n",
      "  GO:0030141       0.00      0.00      0.00       212\n",
      "  GO:0030175       0.00      0.00      0.00        43\n",
      "  GO:0030286       0.00      0.00      0.00        19\n",
      "  GO:0030288       0.00      0.00      0.00        38\n",
      "  GO:0030312       0.04      0.99      0.07       646\n",
      "  GO:0030313       0.00      0.00      0.00        82\n",
      "  GO:0030315       0.00      0.00      0.00        23\n",
      "  GO:0030424       0.02      0.24      0.04       318\n",
      "  GO:0030425       0.06      0.00      0.01       235\n",
      "  GO:0030426       0.00      0.00      0.00        63\n",
      "  GO:0030427       0.00      0.00      0.00       151\n",
      "  GO:0030428       0.00      0.00      0.00        17\n",
      "  GO:0030430       0.00      0.00      0.00        78\n",
      "  GO:0030445       0.00      0.00      0.00         9\n",
      "  GO:0030446       0.00      0.00      0.00        13\n",
      "  GO:0030479       0.00      0.00      0.00        18\n",
      "  GO:0030496       0.00      0.00      0.00        61\n",
      "  GO:0030532       0.00      0.00      0.00        36\n",
      "  GO:0030658       0.00      0.00      0.00        65\n",
      "  GO:0030659       0.02      0.27      0.04       298\n",
      "  GO:0030660       0.00      0.00      0.00        27\n",
      "  GO:0030662       0.00      0.00      0.00        61\n",
      "  GO:0030665       0.00      0.00      0.00        39\n",
      "  GO:0030666       0.00      0.00      0.00        48\n",
      "  GO:0030667       0.00      0.00      0.00        66\n",
      "  GO:0030669       0.00      0.00      0.00        21\n",
      "  GO:0030670       0.00      0.00      0.00        17\n",
      "  GO:0030672       0.00      0.00      0.00        42\n",
      "  GO:0030684       0.00      0.00      0.00        53\n",
      "  GO:0030686       0.00      0.00      0.00        23\n",
      "  GO:0030687       0.00      0.00      0.00         7\n",
      "  GO:0030863       0.00      0.00      0.00       102\n",
      "  GO:0030864       0.00      0.00      0.00        74\n",
      "  GO:0030867       0.00      0.00      0.00         6\n",
      "  GO:0030880       0.00      0.00      0.00       102\n",
      "  GO:0030894       0.00      0.00      0.00        22\n",
      "  GO:0030964       0.00      0.00      0.00        46\n",
      "  GO:0030981       0.00      0.00      0.00        14\n",
      "  GO:0030990       0.00      0.00      0.00        13\n",
      "  GO:0030992       0.00      0.00      0.00        11\n",
      "  GO:0031010       0.00      0.00      0.00         5\n",
      "  GO:0031011       0.00      0.00      0.00        18\n",
      "  GO:0031012       0.02      0.16      0.03       301\n",
      "  GO:0031045       0.00      0.00      0.00        17\n",
      "  GO:0031082       0.00      0.00      0.00        14\n",
      "  GO:0031090       0.08      1.00      0.16      1433\n",
      "  GO:0031091       0.00      0.00      0.00        12\n",
      "  GO:0031093       0.00      0.00      0.00         9\n",
      "  GO:0031097       0.00      0.00      0.00         7\n",
      "  GO:0031143       0.00      0.00      0.00        14\n",
      "  GO:0031160       0.00      0.00      0.00        10\n",
      "  GO:0031201       0.00      0.00      0.00        21\n",
      "  GO:0031234       0.00      0.00      0.00        24\n",
      "  GO:0031248       0.00      0.00      0.00        73\n",
      "  GO:0031252       0.00      0.00      0.00       122\n",
      "  GO:0031253       0.00      0.00      0.00       104\n",
      "  GO:0031256       0.00      0.00      0.00        44\n",
      "  GO:0031261       0.00      0.00      0.00        23\n",
      "  GO:0031312       0.00      0.00      0.00        12\n",
      "  GO:0031410       0.06      1.00      0.11       996\n",
      "  GO:0031430       0.00      0.00      0.00        13\n",
      "  GO:0031461       0.00      0.00      0.00        81\n",
      "  GO:0031463       0.00      0.00      0.00        14\n",
      "  GO:0031514       0.00      0.00      0.00        80\n",
      "  GO:0031519       0.00      0.00      0.00        19\n",
      "  GO:0031526       0.00      0.00      0.00        25\n",
      "  GO:0031594       0.00      0.00      0.00        70\n",
      "  GO:0031672       0.00      0.00      0.00        23\n",
      "  GO:0031674       0.00      0.00      0.00        50\n",
      "  GO:0031901       0.00      0.00      0.00        14\n",
      "  GO:0031902       0.00      0.00      0.00        22\n",
      "  GO:0031903       0.00      0.00      0.00        40\n",
      "  GO:0031907       0.00      0.00      0.00        23\n",
      "  GO:0031941       0.00      0.00      0.00         8\n",
      "  GO:0031965       0.00      0.00      0.00        95\n",
      "  GO:0031966       0.02      0.80      0.04       365\n",
      "  GO:0031967       0.05      1.00      0.09       824\n",
      "  GO:0031968       0.00      0.00      0.00       183\n",
      "  GO:0031969       0.00      0.00      0.00        21\n",
      "  GO:0031970       0.00      0.00      0.00        43\n",
      "  GO:0031974       0.19      1.00      0.31      3156\n",
      "  GO:0031975       0.05      1.00      0.10       906\n",
      "  GO:0031976       0.00      0.00      0.00        93\n",
      "  GO:0031977       0.00      0.00      0.00        15\n",
      "  GO:0031981       0.16      1.00      0.28      2712\n",
      "  GO:0031982       0.08      1.00      0.15      1347\n",
      "  GO:0031983       0.00      0.00      0.00        64\n",
      "  GO:0031984       0.03      0.96      0.06       488\n",
      "  GO:0031985       0.00      0.00      0.00        72\n",
      "  GO:0032010       0.00      0.00      0.00        16\n",
      "  GO:0032040       0.00      0.00      0.00        39\n",
      "  GO:0032153       0.00      0.00      0.00       120\n",
      "  GO:0032154       0.00      0.00      0.00        44\n",
      "  GO:0032156       0.00      0.00      0.00        14\n",
      "  GO:0032279       0.00      0.00      0.00        95\n",
      "  GO:0032391       0.00      0.00      0.00        10\n",
      "  GO:0032420       0.00      0.00      0.00        16\n",
      "  GO:0032421       0.00      0.00      0.00        16\n",
      "  GO:0032432       0.00      0.00      0.00        31\n",
      "  GO:0032541       0.00      0.00      0.00         9\n",
      "  GO:0032587       0.00      0.00      0.00        17\n",
      "  GO:0032588       0.00      0.00      0.00        25\n",
      "  GO:0032589       0.00      0.00      0.00        23\n",
      "  GO:0032590       0.00      0.00      0.00        12\n",
      "  GO:0032806       0.00      0.00      0.00        22\n",
      "  GO:0032809       0.00      0.00      0.00        10\n",
      "  GO:0032838       0.00      0.00      0.00       243\n",
      "  GO:0032991       0.19      1.00      0.32      3265\n",
      "  GO:0032993       0.03      0.97      0.06       533\n",
      "  GO:0033176       0.00      0.00      0.00        19\n",
      "  GO:0033178       0.00      0.00      0.00         9\n",
      "  GO:0033643       0.00      0.00      0.00       157\n",
      "  GO:0033644       0.00      0.00      0.00        46\n",
      "  GO:0033646       0.00      0.00      0.00       104\n",
      "  GO:0033647       0.00      0.00      0.00        45\n",
      "  GO:0033648       0.00      0.00      0.00        43\n",
      "  GO:0033655       0.00      0.00      0.00        53\n",
      "  GO:0034357       0.00      0.00      0.00        81\n",
      "  GO:0034399       0.00      0.00      0.00        75\n",
      "  GO:0034451       0.00      0.00      0.00        45\n",
      "  GO:0034518       0.00      0.00      0.00        16\n",
      "  GO:0034702       0.00      0.00      0.00        91\n",
      "  GO:0034703       0.00      0.00      0.00        59\n",
      "  GO:0034704       0.00      0.00      0.00        19\n",
      "  GO:0034705       0.00      0.00      0.00        29\n",
      "  GO:0034708       0.00      0.00      0.00        43\n",
      "  GO:0034774       0.00      0.00      0.00        63\n",
      "  GO:0035097       0.00      0.00      0.00        27\n",
      "  GO:0035267       0.00      0.00      0.00        12\n",
      "  GO:0035371       0.00      0.00      0.00        15\n",
      "  GO:0035577       0.00      0.00      0.00         7\n",
      "  GO:0035578       0.00      0.00      0.00        17\n",
      "  GO:0035579       0.00      0.00      0.00        21\n",
      "  GO:0035580       0.00      0.00      0.00        14\n",
      "  GO:0035770       0.00      0.00      0.00       141\n",
      "  GO:0035838       0.00      0.00      0.00        12\n",
      "  GO:0035861       0.00      0.00      0.00        37\n",
      "  GO:0035869       0.00      0.00      0.00        45\n",
      "  GO:0036064       0.00      0.00      0.00        84\n",
      "  GO:0036126       0.00      0.00      0.00        51\n",
      "  GO:0036379       0.00      0.00      0.00        21\n",
      "  GO:0036452       0.00      0.00      0.00         8\n",
      "  GO:0036464       0.00      0.00      0.00       131\n",
      "  GO:0036477       0.02      0.80      0.05       403\n",
      "  GO:0042025       0.00      0.00      0.00        31\n",
      "  GO:0042170       0.00      0.00      0.00        94\n",
      "  GO:0042175       0.02      0.48      0.04       311\n",
      "  GO:0042383       0.00      0.00      0.00        60\n",
      "  GO:0042470       0.00      0.00      0.00         7\n",
      "  GO:0042555       0.00      0.00      0.00        15\n",
      "  GO:0042575       0.00      0.00      0.00        22\n",
      "  GO:0042579       0.00      0.00      0.00       209\n",
      "  GO:0042581       0.00      0.00      0.00        36\n",
      "  GO:0042582       0.00      0.00      0.00        23\n",
      "  GO:0042597       0.00      0.00      0.00        45\n",
      "  GO:0042641       0.00      0.00      0.00        26\n",
      "  GO:0042644       0.00      0.00      0.00        11\n",
      "  GO:0042645       0.00      0.00      0.00        26\n",
      "  GO:0042646       0.00      0.00      0.00        11\n",
      "  GO:0042651       0.00      0.00      0.00        77\n",
      "  GO:0042734       0.00      0.00      0.00        68\n",
      "  GO:0042763       0.00      0.00      0.00        18\n",
      "  GO:0042764       0.00      0.00      0.00        17\n",
      "  GO:0042788       0.00      0.00      0.00        32\n",
      "  GO:0042995       0.07      1.00      0.14      1264\n",
      "  GO:0043005       0.03      0.99      0.07       574\n",
      "  GO:0043025       0.01      0.01      0.01       279\n",
      "  GO:0043073       0.00      0.00      0.00        35\n",
      "  GO:0043186       0.00      0.00      0.00        28\n",
      "  GO:0043189       0.00      0.00      0.00        12\n",
      "  GO:0043190       0.00      0.00      0.00        19\n",
      "  GO:0043194       0.00      0.00      0.00         9\n",
      "  GO:0043195       0.00      0.00      0.00        43\n",
      "  GO:0043197       0.00      0.00      0.00        46\n",
      "  GO:0043198       0.00      0.00      0.00        15\n",
      "  GO:0043202       0.00      0.00      0.00        18\n",
      "  GO:0043204       0.00      0.00      0.00        45\n",
      "  GO:0043209       0.00      0.00      0.00        37\n",
      "  GO:0043226       0.68      1.00      0.81     11592\n",
      "  GO:0043227       0.62      1.00      0.77     10557\n",
      "  GO:0043228       0.18      1.00      0.31      3058\n",
      "  GO:0043229       0.66      1.00      0.79     11136\n",
      "  GO:0043230       0.03      0.95      0.05       431\n",
      "  GO:0043231       0.59      1.00      0.74      9992\n",
      "  GO:0043232       0.18      1.00      0.31      3053\n",
      "  GO:0043233       0.19      1.00      0.31      3156\n",
      "  GO:0043235       0.00      0.00      0.00       105\n",
      "  GO:0043292       0.00      0.00      0.00       145\n",
      "  GO:0043296       0.00      0.00      0.00        65\n",
      "  GO:0043332       0.00      0.00      0.00        25\n",
      "  GO:0043596       0.00      0.00      0.00        32\n",
      "  GO:0043601       0.00      0.00      0.00        17\n",
      "  GO:0043656       0.00      0.00      0.00       104\n",
      "  GO:0043657       0.00      0.00      0.00       159\n",
      "  GO:0043679       0.00      0.00      0.00        80\n",
      "  GO:0044232       0.00      0.00      0.00        23\n",
      "  GO:0044291       0.00      0.00      0.00        23\n",
      "  GO:0044295       0.00      0.00      0.00        14\n",
      "  GO:0044297       0.02      0.07      0.03       307\n",
      "  GO:0044298       0.00      0.00      0.00        14\n",
      "  GO:0044304       0.00      0.00      0.00        33\n",
      "  GO:0044306       0.00      0.00      0.00        83\n",
      "  GO:0044309       0.00      0.00      0.00        47\n",
      "  GO:0044391       0.04      0.01      0.02       248\n",
      "  GO:0044423       0.00      0.00      0.00        30\n",
      "  GO:0044732       0.00      0.00      0.00        30\n",
      "  GO:0044853       0.00      0.00      0.00        44\n",
      "  GO:0045111       0.00      0.00      0.00        49\n",
      "  GO:0045121       0.00      0.00      0.00       104\n",
      "  GO:0045169       0.00      0.00      0.00        21\n",
      "  GO:0045171       0.00      0.00      0.00        42\n",
      "  GO:0045172       0.00      0.00      0.00         9\n",
      "  GO:0045177       0.01      0.01      0.01       222\n",
      "  GO:0045178       0.00      0.00      0.00       138\n",
      "  GO:0045179       0.00      0.00      0.00        26\n",
      "  GO:0045202       0.04      0.99      0.07       592\n",
      "  GO:0045211       0.00      0.00      0.00        89\n",
      "  GO:0045239       0.00      0.00      0.00         7\n",
      "  GO:0045259       0.00      0.00      0.00        28\n",
      "  GO:0045271       0.00      0.00      0.00        39\n",
      "  GO:0045273       0.00      0.00      0.00         4\n",
      "  GO:0045275       0.00      0.00      0.00        11\n",
      "  GO:0045277       0.00      0.00      0.00        16\n",
      "  GO:0045293       0.00      0.00      0.00        15\n",
      "  GO:0045334       0.00      0.00      0.00        23\n",
      "  GO:0045335       0.00      0.00      0.00       111\n",
      "  GO:0045495       0.00      0.00      0.00        32\n",
      "  GO:0046540       0.00      0.00      0.00        11\n",
      "  GO:0048046       0.00      0.00      0.00        44\n",
      "  GO:0048471       0.06      0.01      0.02       225\n",
      "  GO:0048475       0.00      0.00      0.00        44\n",
      "  GO:0048770       0.00      0.00      0.00         7\n",
      "  GO:0048786       0.00      0.00      0.00        43\n",
      "  GO:0048787       0.00      0.00      0.00        14\n",
      "  GO:0051233       0.00      0.00      0.00        27\n",
      "  GO:0051285       0.00      0.00      0.00        10\n",
      "  GO:0051286       0.00      0.00      0.00       105\n",
      "  GO:0055028       0.00      0.00      0.00        13\n",
      "  GO:0055029       0.00      0.00      0.00        84\n",
      "  GO:0055035       0.00      0.00      0.00        71\n",
      "  GO:0055037       0.00      0.00      0.00        69\n",
      "  GO:0055038       0.00      0.00      0.00        18\n",
      "  GO:0055044       0.00      0.00      0.00       172\n",
      "  GO:0055120       0.00      0.00      0.00        29\n",
      "  GO:0060076       0.00      0.00      0.00        34\n",
      "  GO:0060170       0.00      0.00      0.00        26\n",
      "  GO:0060187       0.00      0.00      0.00       113\n",
      "  GO:0060205       0.00      0.00      0.00        63\n",
      "  GO:0060293       0.00      0.00      0.00        29\n",
      "  GO:0061174       0.00      0.00      0.00        14\n",
      "  GO:0061645       0.00      0.00      0.00        20\n",
      "  GO:0061695       0.00      0.00      0.00       219\n",
      "  GO:0062023       0.00      0.00      0.00       173\n",
      "  GO:0062039       0.00      0.00      0.00        66\n",
      "  GO:0062040       0.00      0.00      0.00        66\n",
      "  GO:0065010       0.03      0.94      0.05       431\n",
      "  GO:0070013       0.19      1.00      0.31      3156\n",
      "  GO:0070062       0.02      0.90      0.05       392\n",
      "  GO:0070069       0.00      0.00      0.00        32\n",
      "  GO:0070160       0.00      0.00      0.00        52\n",
      "  GO:0070161       0.03      0.98      0.06       520\n",
      "  GO:0070382       0.00      0.00      0.00       100\n",
      "  GO:0070461       0.00      0.00      0.00        35\n",
      "  GO:0070469       0.00      0.00      0.00        72\n",
      "  GO:0070603       0.00      0.00      0.00        72\n",
      "  GO:0070820       0.00      0.00      0.00        29\n",
      "  GO:0070821       0.00      0.00      0.00        17\n",
      "  GO:0070822       0.00      0.00      0.00         7\n",
      "  GO:0070847       0.00      0.00      0.00         7\n",
      "  GO:0070938       0.00      0.00      0.00        31\n",
      "  GO:0070971       0.00      0.00      0.00        11\n",
      "  GO:0071011       0.00      0.00      0.00        69\n",
      "  GO:0071013       0.00      0.00      0.00        58\n",
      "  GO:0071014       0.00      0.00      0.00        10\n",
      "  GO:0071162       0.00      0.00      0.00        15\n",
      "  GO:0071782       0.00      0.00      0.00        14\n",
      "  GO:0071944       0.23      1.00      0.37      3846\n",
      "  GO:0072562       0.00      0.00      0.00        27\n",
      "  GO:0072686       0.00      0.00      0.00        96\n",
      "  GO:0072687       0.00      0.00      0.00        12\n",
      "  GO:0080008       0.00      0.00      0.00        13\n",
      "  GO:0090406       0.00      0.00      0.00        12\n",
      "  GO:0090545       0.00      0.00      0.00        10\n",
      "  GO:0090571       0.00      0.00      0.00         9\n",
      "  GO:0090575       0.00      0.00      0.00       107\n",
      "  GO:0090665       0.00      0.00      0.00        10\n",
      "  GO:0090734       0.00      0.00      0.00        43\n",
      "  GO:0097014       0.00      0.00      0.00       226\n",
      "  GO:0097038       0.00      0.00      0.00         9\n",
      "  GO:0097060       0.00      0.00      0.00       139\n",
      "  GO:0097225       0.00      0.00      0.00        11\n",
      "  GO:0097346       0.00      0.00      0.00        25\n",
      "  GO:0097386       0.00      0.00      0.00        10\n",
      "  GO:0097447       0.06      0.00      0.01       236\n",
      "  GO:0097517       0.00      0.00      0.00        23\n",
      "  GO:0097525       0.00      0.00      0.00        32\n",
      "  GO:0097526       0.00      0.00      0.00        11\n",
      "  GO:0097542       0.00      0.00      0.00        16\n",
      "  GO:0097546       0.00      0.00      0.00        23\n",
      "  GO:0097708       0.06      1.00      0.11      1001\n",
      "  GO:0097729       0.00      0.00      0.00        59\n",
      "  GO:0097730       0.00      0.00      0.00        83\n",
      "  GO:0097731       0.00      0.00      0.00        49\n",
      "  GO:0097733       0.00      0.00      0.00        48\n",
      "  GO:0098533       0.00      0.00      0.00        24\n",
      "  GO:0098552       0.00      0.00      0.00       222\n",
      "  GO:0098554       0.00      0.00      0.00         7\n",
      "  GO:0098562       0.00      0.00      0.00        97\n",
      "  GO:0098576       0.00      0.00      0.00         9\n",
      "  GO:0098588       0.05      1.00      0.09       808\n",
      "  GO:0098590       0.03      0.98      0.07       567\n",
      "  GO:0098636       0.00      0.00      0.00        19\n",
      "  GO:0098644       0.00      0.00      0.00        10\n",
      "  GO:0098685       0.00      0.00      0.00        25\n",
      "  GO:0098686       0.00      0.00      0.00        15\n",
      "  GO:0098687       0.00      0.00      0.00       244\n",
      "  GO:0098688       0.00      0.00      0.00        13\n",
      "  GO:0098791       0.00      0.00      0.00       182\n",
      "  GO:0098793       0.02      0.01      0.02       263\n",
      "  GO:0098794       0.00      0.00      0.00       222\n",
      "  GO:0098796       0.03      0.99      0.07       587\n",
      "  GO:0098797       0.01      0.00      0.01       254\n",
      "  GO:0098798       0.05      0.01      0.02       230\n",
      "  GO:0098799       0.00      0.00      0.00        13\n",
      "  GO:0098800       0.00      0.00      0.00       106\n",
      "  GO:0098802       0.00      0.00      0.00        59\n",
      "  GO:0098803       0.00      0.00      0.00        69\n",
      "  GO:0098827       0.02      0.48      0.03       313\n",
      "  GO:0098839       0.00      0.00      0.00        26\n",
      "  GO:0098852       0.00      0.00      0.00       162\n",
      "  GO:0098857       0.00      0.00      0.00       104\n",
      "  GO:0098858       0.00      0.00      0.00        82\n",
      "  GO:0098862       0.00      0.00      0.00        62\n",
      "  GO:0098878       0.00      0.00      0.00        14\n",
      "  GO:0098978       0.00      0.00      0.00       184\n",
      "  GO:0098982       0.00      0.00      0.00        29\n",
      "  GO:0098984       0.00      0.00      0.00       106\n",
      "  GO:0099023       0.00      0.00      0.00        28\n",
      "  GO:0099080       0.04      0.99      0.07       601\n",
      "  GO:0099081       0.02      0.74      0.04       374\n",
      "  GO:0099086       0.00      0.00      0.00        26\n",
      "  GO:0099091       0.00      0.00      0.00        14\n",
      "  GO:0099092       0.00      0.00      0.00        12\n",
      "  GO:0099501       0.00      0.00      0.00        42\n",
      "  GO:0099503       0.02      0.53      0.04       337\n",
      "  GO:0099512       0.02      0.75      0.04       372\n",
      "  GO:0099513       0.00      0.00      0.00       218\n",
      "  GO:0099522       0.00      0.00      0.00        11\n",
      "  GO:0099568       0.02      0.31      0.04       296\n",
      "  GO:0099572       0.00      0.00      0.00       110\n",
      "  GO:0099634       0.00      0.00      0.00        36\n",
      "  GO:0099738       0.00      0.00      0.00        54\n",
      "  GO:0101002       0.00      0.00      0.00        33\n",
      "  GO:0101003       0.00      0.00      0.00        10\n",
      "  GO:0101031       0.00      0.00      0.00        23\n",
      "  GO:0110085       0.00      0.00      0.00        13\n",
      "  GO:0110165       0.99      1.00      1.00     16782\n",
      "  GO:0120025       0.07      1.00      0.14      1240\n",
      "  GO:0120111       0.00      0.00      0.00        19\n",
      "  GO:0120114       0.00      0.00      0.00        50\n",
      "  GO:0120119       0.00      0.00      0.00        13\n",
      "  GO:0140220       0.00      0.00      0.00         6\n",
      "  GO:0140445       0.00      0.00      0.00        17\n",
      "  GO:0140513       0.05      0.99      0.09       769\n",
      "  GO:0140534       0.00      0.00      0.00        57\n",
      "  GO:0140535       0.04      0.97      0.07       598\n",
      "  GO:0150034       0.00      0.00      0.00       139\n",
      "  GO:1902493       0.00      0.00      0.00        74\n",
      "  GO:1902494       0.07      1.00      0.12      1113\n",
      "  GO:1902495       0.00      0.00      0.00       178\n",
      "  GO:1902554       0.00      0.00      0.00        75\n",
      "  GO:1902555       0.00      0.00      0.00        13\n",
      "  GO:1902562       0.00      0.00      0.00        25\n",
      "  GO:1902911       0.00      0.00      0.00        88\n",
      "  GO:1903293       0.00      0.00      0.00        26\n",
      "  GO:1903561       0.02      0.91      0.05       410\n",
      "  GO:1904724       0.00      0.00      0.00         9\n",
      "  GO:1904813       0.00      0.00      0.00        23\n",
      "  GO:1904949       0.00      0.00      0.00       121\n",
      "  GO:1905348       0.00      0.00      0.00        22\n",
      "  GO:1905354       0.00      0.00      0.00        11\n",
      "  GO:1905360       0.00      0.00      0.00        22\n",
      "  GO:1905368       0.00      0.00      0.00       109\n",
      "  GO:1905369       0.00      0.00      0.00        77\n",
      "  GO:1990023       0.00      0.00      0.00        16\n",
      "  GO:1990204       0.00      0.00      0.00        96\n",
      "  GO:1990234       0.03      0.94      0.06       511\n",
      "  GO:1990351       0.00      0.00      0.00       197\n",
      "  GO:1990752       0.00      0.00      0.00        19\n",
      "  GO:1990904       0.03      0.96      0.06       539\n",
      "\n",
      "   micro avg       0.15      0.83      0.25    221564\n",
      "   macro avg       0.02      0.11      0.02    221564\n",
      "weighted avg       0.41      0.83      0.48    221564\n",
      " samples avg       0.15      0.87      0.24    221564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=mlb.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - auc: 0.8176 - loss: 0.1829 - precision_6: 0.2209 - recall_6: 0.4351 - val_auc: 0.9151 - val_loss: 0.0510 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 2/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9072 - loss: 0.0616 - precision_6: 0.7438 - recall_6: 0.4499 - val_auc: 0.9206 - val_loss: 0.0506 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 3/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9099 - loss: 0.0614 - precision_6: 0.7432 - recall_6: 0.4500 - val_auc: 0.9206 - val_loss: 0.0502 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 4/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - auc: 0.9117 - loss: 0.0606 - precision_6: 0.7420 - recall_6: 0.4512 - val_auc: 0.9226 - val_loss: 0.0500 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 5/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9122 - loss: 0.0594 - precision_6: 0.7430 - recall_6: 0.4487 - val_auc: 0.9228 - val_loss: 0.0506 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 6/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9143 - loss: 0.0593 - precision_6: 0.7448 - recall_6: 0.4504 - val_auc: 0.9216 - val_loss: 0.0504 - val_precision_6: 0.7431 - val_recall_6: 0.4542\n",
      "Epoch 7/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - auc: 0.9152 - loss: 0.0585 - precision_6: 0.7419 - recall_6: 0.4513 - val_auc: 0.9228 - val_loss: 0.0503 - val_precision_6: 0.7434 - val_recall_6: 0.4534\n",
      "Epoch 8/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9161 - loss: 0.0587 - precision_6: 0.7457 - recall_6: 0.4504 - val_auc: 0.9246 - val_loss: 0.0499 - val_precision_6: 0.7442 - val_recall_6: 0.4511\n",
      "Epoch 9/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9167 - loss: 0.0578 - precision_6: 0.7416 - recall_6: 0.4493 - val_auc: 0.9241 - val_loss: 0.0500 - val_precision_6: 0.7431 - val_recall_6: 0.4541\n",
      "Epoch 10/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9176 - loss: 0.0571 - precision_6: 0.7411 - recall_6: 0.4482 - val_auc: 0.9251 - val_loss: 0.0503 - val_precision_6: 0.7422 - val_recall_6: 0.4541\n",
      "Epoch 11/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9179 - loss: 0.0564 - precision_6: 0.7370 - recall_6: 0.4510 - val_auc: 0.9244 - val_loss: 0.0498 - val_precision_6: 0.7432 - val_recall_6: 0.4525\n",
      "Epoch 12/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9189 - loss: 0.0563 - precision_6: 0.7345 - recall_6: 0.4504 - val_auc: 0.9244 - val_loss: 0.0495 - val_precision_6: 0.7429 - val_recall_6: 0.4536\n",
      "Epoch 13/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9193 - loss: 0.0566 - precision_6: 0.7335 - recall_6: 0.4509 - val_auc: 0.9253 - val_loss: 0.0495 - val_precision_6: 0.7420 - val_recall_6: 0.4538\n",
      "Epoch 14/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9192 - loss: 0.0567 - precision_6: 0.7322 - recall_6: 0.4517 - val_auc: 0.9277 - val_loss: 0.0495 - val_precision_6: 0.7405 - val_recall_6: 0.4538\n",
      "Epoch 15/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9205 - loss: 0.0553 - precision_6: 0.7324 - recall_6: 0.4540 - val_auc: 0.9268 - val_loss: 0.0493 - val_precision_6: 0.7410 - val_recall_6: 0.4539\n",
      "Epoch 16/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9203 - loss: 0.0552 - precision_6: 0.7288 - recall_6: 0.4514 - val_auc: 0.9263 - val_loss: 0.0494 - val_precision_6: 0.7399 - val_recall_6: 0.4541\n",
      "Epoch 17/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9206 - loss: 0.0548 - precision_6: 0.7308 - recall_6: 0.4532 - val_auc: 0.9262 - val_loss: 0.0494 - val_precision_6: 0.7387 - val_recall_6: 0.4542\n",
      "Epoch 18/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9203 - loss: 0.0550 - precision_6: 0.7305 - recall_6: 0.4523 - val_auc: 0.9271 - val_loss: 0.0493 - val_precision_6: 0.7383 - val_recall_6: 0.4540\n",
      "Epoch 19/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9207 - loss: 0.0553 - precision_6: 0.7325 - recall_6: 0.4544 - val_auc: 0.9266 - val_loss: 0.0493 - val_precision_6: 0.7386 - val_recall_6: 0.4540\n",
      "Epoch 20/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9210 - loss: 0.0552 - precision_6: 0.7275 - recall_6: 0.4539 - val_auc: 0.9263 - val_loss: 0.0493 - val_precision_6: 0.7394 - val_recall_6: 0.4541\n",
      "Epoch 21/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9213 - loss: 0.0544 - precision_6: 0.7299 - recall_6: 0.4527 - val_auc: 0.9274 - val_loss: 0.0494 - val_precision_6: 0.7353 - val_recall_6: 0.4542\n",
      "Epoch 22/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9221 - loss: 0.0541 - precision_6: 0.7297 - recall_6: 0.4535 - val_auc: 0.9272 - val_loss: 0.0493 - val_precision_6: 0.7374 - val_recall_6: 0.4545\n",
      "Epoch 23/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9220 - loss: 0.0537 - precision_6: 0.7303 - recall_6: 0.4552 - val_auc: 0.9257 - val_loss: 0.0492 - val_precision_6: 0.7394 - val_recall_6: 0.4541\n",
      "Epoch 24/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9217 - loss: 0.0536 - precision_6: 0.7318 - recall_6: 0.4532 - val_auc: 0.9264 - val_loss: 0.0494 - val_precision_6: 0.7365 - val_recall_6: 0.4544\n",
      "Epoch 25/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9228 - loss: 0.0531 - precision_6: 0.7330 - recall_6: 0.4565 - val_auc: 0.9259 - val_loss: 0.0494 - val_precision_6: 0.7373 - val_recall_6: 0.4544\n",
      "Epoch 26/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9221 - loss: 0.0536 - precision_6: 0.7336 - recall_6: 0.4554 - val_auc: 0.9267 - val_loss: 0.0493 - val_precision_6: 0.7373 - val_recall_6: 0.4540\n",
      "Epoch 27/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9231 - loss: 0.0533 - precision_6: 0.7311 - recall_6: 0.4558 - val_auc: 0.9265 - val_loss: 0.0493 - val_precision_6: 0.7384 - val_recall_6: 0.4542\n",
      "Epoch 28/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9228 - loss: 0.0533 - precision_6: 0.7322 - recall_6: 0.4554 - val_auc: 0.9269 - val_loss: 0.0492 - val_precision_6: 0.7377 - val_recall_6: 0.4542\n",
      "Epoch 29/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9232 - loss: 0.0538 - precision_6: 0.7290 - recall_6: 0.4560 - val_auc: 0.9262 - val_loss: 0.0492 - val_precision_6: 0.7383 - val_recall_6: 0.4543\n",
      "Epoch 30/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9228 - loss: 0.0534 - precision_6: 0.7333 - recall_6: 0.4543 - val_auc: 0.9271 - val_loss: 0.0492 - val_precision_6: 0.7381 - val_recall_6: 0.4541\n",
      "Epoch 31/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9232 - loss: 0.0533 - precision_6: 0.7340 - recall_6: 0.4544 - val_auc: 0.9265 - val_loss: 0.0493 - val_precision_6: 0.7386 - val_recall_6: 0.4544\n",
      "Epoch 32/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9235 - loss: 0.0525 - precision_6: 0.7364 - recall_6: 0.4562 - val_auc: 0.9280 - val_loss: 0.0492 - val_precision_6: 0.7389 - val_recall_6: 0.4543\n",
      "Epoch 33/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9237 - loss: 0.0526 - precision_6: 0.7344 - recall_6: 0.4563 - val_auc: 0.9278 - val_loss: 0.0493 - val_precision_6: 0.7369 - val_recall_6: 0.4543\n",
      "Epoch 34/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9236 - loss: 0.0532 - precision_6: 0.7321 - recall_6: 0.4549 - val_auc: 0.9266 - val_loss: 0.0492 - val_precision_6: 0.7393 - val_recall_6: 0.4542\n",
      "Epoch 35/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9236 - loss: 0.0530 - precision_6: 0.7353 - recall_6: 0.4540 - val_auc: 0.9278 - val_loss: 0.0492 - val_precision_6: 0.7399 - val_recall_6: 0.4541\n",
      "Epoch 36/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9241 - loss: 0.0526 - precision_6: 0.7341 - recall_6: 0.4571 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7377 - val_recall_6: 0.4542\n",
      "Epoch 37/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9244 - loss: 0.0531 - precision_6: 0.7353 - recall_6: 0.4537 - val_auc: 0.9282 - val_loss: 0.0492 - val_precision_6: 0.7382 - val_recall_6: 0.4544\n",
      "Epoch 38/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9243 - loss: 0.0533 - precision_6: 0.7327 - recall_6: 0.4556 - val_auc: 0.9281 - val_loss: 0.0492 - val_precision_6: 0.7390 - val_recall_6: 0.4543\n",
      "Epoch 39/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9241 - loss: 0.0528 - precision_6: 0.7345 - recall_6: 0.4546 - val_auc: 0.9280 - val_loss: 0.0492 - val_precision_6: 0.7386 - val_recall_6: 0.4543\n",
      "Epoch 40/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9247 - loss: 0.0532 - precision_6: 0.7349 - recall_6: 0.4556 - val_auc: 0.9272 - val_loss: 0.0492 - val_precision_6: 0.7384 - val_recall_6: 0.4544\n",
      "Epoch 41/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9249 - loss: 0.0526 - precision_6: 0.7361 - recall_6: 0.4560 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7393 - val_recall_6: 0.4543\n",
      "Epoch 42/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9247 - loss: 0.0528 - precision_6: 0.7351 - recall_6: 0.4550 - val_auc: 0.9264 - val_loss: 0.0493 - val_precision_6: 0.7383 - val_recall_6: 0.4543\n",
      "Epoch 43/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9246 - loss: 0.0523 - precision_6: 0.7340 - recall_6: 0.4553 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7402 - val_recall_6: 0.4544\n",
      "Epoch 44/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9249 - loss: 0.0524 - precision_6: 0.7377 - recall_6: 0.4566 - val_auc: 0.9284 - val_loss: 0.0493 - val_precision_6: 0.7383 - val_recall_6: 0.4543\n",
      "Epoch 45/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9254 - loss: 0.0528 - precision_6: 0.7373 - recall_6: 0.4552 - val_auc: 0.9267 - val_loss: 0.0492 - val_precision_6: 0.7387 - val_recall_6: 0.4542\n",
      "Epoch 46/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9253 - loss: 0.0522 - precision_6: 0.7344 - recall_6: 0.4562 - val_auc: 0.9285 - val_loss: 0.0492 - val_precision_6: 0.7387 - val_recall_6: 0.4543\n",
      "Epoch 47/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9251 - loss: 0.0527 - precision_6: 0.7357 - recall_6: 0.4557 - val_auc: 0.9281 - val_loss: 0.0492 - val_precision_6: 0.7383 - val_recall_6: 0.4543\n",
      "Epoch 48/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9259 - loss: 0.0519 - precision_6: 0.7388 - recall_6: 0.4573 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7386 - val_recall_6: 0.4542\n",
      "Epoch 49/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9259 - loss: 0.0519 - precision_6: 0.7357 - recall_6: 0.4553 - val_auc: 0.9283 - val_loss: 0.0492 - val_precision_6: 0.7401 - val_recall_6: 0.4543\n",
      "Epoch 50/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9257 - loss: 0.0521 - precision_6: 0.7359 - recall_6: 0.4546 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7387 - val_recall_6: 0.4542\n",
      "Epoch 51/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9259 - loss: 0.0528 - precision_6: 0.7379 - recall_6: 0.4562 - val_auc: 0.9279 - val_loss: 0.0492 - val_precision_6: 0.7401 - val_recall_6: 0.4542\n",
      "Epoch 52/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9257 - loss: 0.0524 - precision_6: 0.7359 - recall_6: 0.4547 - val_auc: 0.9268 - val_loss: 0.0492 - val_precision_6: 0.7395 - val_recall_6: 0.4544\n",
      "Epoch 53/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9257 - loss: 0.0522 - precision_6: 0.7345 - recall_6: 0.4546 - val_auc: 0.9277 - val_loss: 0.0492 - val_precision_6: 0.7398 - val_recall_6: 0.4542\n",
      "Epoch 54/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9259 - loss: 0.0518 - precision_6: 0.7367 - recall_6: 0.4548 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7384 - val_recall_6: 0.4543\n",
      "Epoch 55/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9261 - loss: 0.0525 - precision_6: 0.7373 - recall_6: 0.4567 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7384 - val_recall_6: 0.4543\n",
      "Epoch 56/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9253 - loss: 0.0526 - precision_6: 0.7371 - recall_6: 0.4545 - val_auc: 0.9274 - val_loss: 0.0493 - val_precision_6: 0.7391 - val_recall_6: 0.4543\n",
      "Epoch 57/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9254 - loss: 0.0523 - precision_6: 0.7365 - recall_6: 0.4552 - val_auc: 0.9281 - val_loss: 0.0492 - val_precision_6: 0.7390 - val_recall_6: 0.4543\n",
      "Epoch 58/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9262 - loss: 0.0518 - precision_6: 0.7387 - recall_6: 0.4561 - val_auc: 0.9280 - val_loss: 0.0492 - val_precision_6: 0.7390 - val_recall_6: 0.4544\n",
      "Epoch 59/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9264 - loss: 0.0520 - precision_6: 0.7382 - recall_6: 0.4547 - val_auc: 0.9279 - val_loss: 0.0492 - val_precision_6: 0.7395 - val_recall_6: 0.4544\n",
      "Epoch 60/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9268 - loss: 0.0528 - precision_6: 0.7380 - recall_6: 0.4562 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7394 - val_recall_6: 0.4543\n",
      "Epoch 61/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9261 - loss: 0.0523 - precision_6: 0.7350 - recall_6: 0.4538 - val_auc: 0.9284 - val_loss: 0.0492 - val_precision_6: 0.7391 - val_recall_6: 0.4544\n",
      "Epoch 62/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9264 - loss: 0.0518 - precision_6: 0.7373 - recall_6: 0.4548 - val_auc: 0.9282 - val_loss: 0.0493 - val_precision_6: 0.7382 - val_recall_6: 0.4544\n",
      "Epoch 63/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9266 - loss: 0.0518 - precision_6: 0.7362 - recall_6: 0.4565 - val_auc: 0.9283 - val_loss: 0.0493 - val_precision_6: 0.7390 - val_recall_6: 0.4543\n",
      "Epoch 64/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9271 - loss: 0.0524 - precision_6: 0.7380 - recall_6: 0.4559 - val_auc: 0.9277 - val_loss: 0.0492 - val_precision_6: 0.7399 - val_recall_6: 0.4543\n",
      "Epoch 65/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9265 - loss: 0.0524 - precision_6: 0.7378 - recall_6: 0.4561 - val_auc: 0.9292 - val_loss: 0.0492 - val_precision_6: 0.7403 - val_recall_6: 0.4543\n",
      "Epoch 66/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9272 - loss: 0.0517 - precision_6: 0.7396 - recall_6: 0.4565 - val_auc: 0.9281 - val_loss: 0.0492 - val_precision_6: 0.7398 - val_recall_6: 0.4543\n",
      "Epoch 67/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9269 - loss: 0.0519 - precision_6: 0.7366 - recall_6: 0.4551 - val_auc: 0.9279 - val_loss: 0.0493 - val_precision_6: 0.7387 - val_recall_6: 0.4543\n",
      "Epoch 68/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9270 - loss: 0.0517 - precision_6: 0.7376 - recall_6: 0.4567 - val_auc: 0.9288 - val_loss: 0.0493 - val_precision_6: 0.7387 - val_recall_6: 0.4543\n",
      "Epoch 69/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9265 - loss: 0.0515 - precision_6: 0.7372 - recall_6: 0.4544 - val_auc: 0.9279 - val_loss: 0.0493 - val_precision_6: 0.7398 - val_recall_6: 0.4544\n",
      "Epoch 70/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9270 - loss: 0.0519 - precision_6: 0.7393 - recall_6: 0.4555 - val_auc: 0.9284 - val_loss: 0.0491 - val_precision_6: 0.7406 - val_recall_6: 0.4543\n",
      "Epoch 71/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9275 - loss: 0.0517 - precision_6: 0.7371 - recall_6: 0.4571 - val_auc: 0.9299 - val_loss: 0.0493 - val_precision_6: 0.7394 - val_recall_6: 0.4543\n",
      "Epoch 72/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9271 - loss: 0.0517 - precision_6: 0.7373 - recall_6: 0.4572 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7402 - val_recall_6: 0.4543\n",
      "Epoch 73/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9274 - loss: 0.0516 - precision_6: 0.7375 - recall_6: 0.4558 - val_auc: 0.9280 - val_loss: 0.0492 - val_precision_6: 0.7394 - val_recall_6: 0.4543\n",
      "Epoch 74/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9279 - loss: 0.0515 - precision_6: 0.7379 - recall_6: 0.4578 - val_auc: 0.9283 - val_loss: 0.0492 - val_precision_6: 0.7390 - val_recall_6: 0.4544\n",
      "Epoch 75/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9275 - loss: 0.0515 - precision_6: 0.7386 - recall_6: 0.4555 - val_auc: 0.9292 - val_loss: 0.0492 - val_precision_6: 0.7393 - val_recall_6: 0.4543\n",
      "Epoch 76/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9273 - loss: 0.0514 - precision_6: 0.7382 - recall_6: 0.4559 - val_auc: 0.9284 - val_loss: 0.0492 - val_precision_6: 0.7400 - val_recall_6: 0.4543\n",
      "Epoch 77/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9275 - loss: 0.0519 - precision_6: 0.7385 - recall_6: 0.4558 - val_auc: 0.9282 - val_loss: 0.0492 - val_precision_6: 0.7398 - val_recall_6: 0.4544\n",
      "Epoch 78/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9276 - loss: 0.0516 - precision_6: 0.7374 - recall_6: 0.4563 - val_auc: 0.9283 - val_loss: 0.0492 - val_precision_6: 0.7404 - val_recall_6: 0.4538\n",
      "Epoch 79/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9277 - loss: 0.0517 - precision_6: 0.7393 - recall_6: 0.4576 - val_auc: 0.9289 - val_loss: 0.0492 - val_precision_6: 0.7405 - val_recall_6: 0.4543\n",
      "Epoch 80/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9280 - loss: 0.0518 - precision_6: 0.7379 - recall_6: 0.4564 - val_auc: 0.9291 - val_loss: 0.0492 - val_precision_6: 0.7403 - val_recall_6: 0.4544\n",
      "Epoch 81/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9279 - loss: 0.0516 - precision_6: 0.7386 - recall_6: 0.4572 - val_auc: 0.9290 - val_loss: 0.0492 - val_precision_6: 0.7403 - val_recall_6: 0.4543\n",
      "Epoch 82/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9276 - loss: 0.0518 - precision_6: 0.7377 - recall_6: 0.4559 - val_auc: 0.9281 - val_loss: 0.0492 - val_precision_6: 0.7400 - val_recall_6: 0.4543\n",
      "Epoch 83/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.9279 - loss: 0.0516 - precision_6: 0.7352 - recall_6: 0.4553 - val_auc: 0.9296 - val_loss: 0.0492 - val_precision_6: 0.7399 - val_recall_6: 0.4543\n",
      "Epoch 84/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9276 - loss: 0.0518 - precision_6: 0.7382 - recall_6: 0.4555 - val_auc: 0.9289 - val_loss: 0.0492 - val_precision_6: 0.7397 - val_recall_6: 0.4543\n",
      "Epoch 85/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9280 - loss: 0.0516 - precision_6: 0.7379 - recall_6: 0.4572 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7401 - val_recall_6: 0.4543\n",
      "Epoch 86/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9280 - loss: 0.0513 - precision_6: 0.7380 - recall_6: 0.4563 - val_auc: 0.9289 - val_loss: 0.0492 - val_precision_6: 0.7401 - val_recall_6: 0.4543\n",
      "Epoch 87/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9280 - loss: 0.0516 - precision_6: 0.7374 - recall_6: 0.4554 - val_auc: 0.9294 - val_loss: 0.0492 - val_precision_6: 0.7398 - val_recall_6: 0.4543\n",
      "Epoch 88/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9277 - loss: 0.0516 - precision_6: 0.7386 - recall_6: 0.4547 - val_auc: 0.9290 - val_loss: 0.0492 - val_precision_6: 0.7400 - val_recall_6: 0.4543\n",
      "Epoch 89/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9284 - loss: 0.0514 - precision_6: 0.7389 - recall_6: 0.4554 - val_auc: 0.9288 - val_loss: 0.0493 - val_precision_6: 0.7401 - val_recall_6: 0.4543\n",
      "Epoch 90/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9282 - loss: 0.0510 - precision_6: 0.7384 - recall_6: 0.4552 - val_auc: 0.9297 - val_loss: 0.0493 - val_precision_6: 0.7396 - val_recall_6: 0.4544\n",
      "Epoch 91/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9285 - loss: 0.0512 - precision_6: 0.7383 - recall_6: 0.4546 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7404 - val_recall_6: 0.4543\n",
      "Epoch 92/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9287 - loss: 0.0517 - precision_6: 0.7376 - recall_6: 0.4570 - val_auc: 0.9294 - val_loss: 0.0492 - val_precision_6: 0.7404 - val_recall_6: 0.4543\n",
      "Epoch 93/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9281 - loss: 0.0523 - precision_6: 0.7384 - recall_6: 0.4557 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7405 - val_recall_6: 0.4543\n",
      "Epoch 94/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9287 - loss: 0.0510 - precision_6: 0.7398 - recall_6: 0.4562 - val_auc: 0.9290 - val_loss: 0.0492 - val_precision_6: 0.7399 - val_recall_6: 0.4544\n",
      "Epoch 95/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9287 - loss: 0.0513 - precision_6: 0.7396 - recall_6: 0.4572 - val_auc: 0.9290 - val_loss: 0.0492 - val_precision_6: 0.7402 - val_recall_6: 0.4544\n",
      "Epoch 96/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9289 - loss: 0.0514 - precision_6: 0.7397 - recall_6: 0.4553 - val_auc: 0.9290 - val_loss: 0.0492 - val_precision_6: 0.7400 - val_recall_6: 0.4542\n",
      "Epoch 97/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9286 - loss: 0.0511 - precision_6: 0.7383 - recall_6: 0.4563 - val_auc: 0.9287 - val_loss: 0.0492 - val_precision_6: 0.7402 - val_recall_6: 0.4544\n",
      "Epoch 98/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9281 - loss: 0.0516 - precision_6: 0.7379 - recall_6: 0.4540 - val_auc: 0.9284 - val_loss: 0.0492 - val_precision_6: 0.7397 - val_recall_6: 0.4544\n",
      "Epoch 99/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9285 - loss: 0.0510 - precision_6: 0.7397 - recall_6: 0.4567 - val_auc: 0.9288 - val_loss: 0.0492 - val_precision_6: 0.7401 - val_recall_6: 0.4543\n",
      "Epoch 100/100\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - auc: 0.9287 - loss: 0.0515 - precision_6: 0.7394 - recall_6: 0.4559 - val_auc: 0.9289 - val_loss: 0.0492 - val_precision_6: 0.7403 - val_recall_6: 0.4543\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train.flatten())\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[Precision(), Recall(), AUC(name='auc')])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test_scaled, y_test),\n",
    "                    class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  GO:0000118       0.00      0.00      0.00        44\n",
      "  GO:0000123       0.00      0.00      0.00        67\n",
      "  GO:0000124       0.00      0.00      0.00        23\n",
      "  GO:0000131       0.00      0.00      0.00        15\n",
      "  GO:0000137       0.00      0.00      0.00        20\n",
      "  GO:0000138       0.00      0.00      0.00        25\n",
      "  GO:0000139       0.00      0.00      0.00       133\n",
      "  GO:0000145       0.00      0.00      0.00         8\n",
      "  GO:0000151       0.00      0.00      0.00       131\n",
      "  GO:0000152       0.00      0.00      0.00        30\n",
      "  GO:0000176       0.00      0.00      0.00         5\n",
      "  GO:0000178       0.00      0.00      0.00        11\n",
      "  GO:0000228       0.00      0.00      0.00       159\n",
      "  GO:0000235       0.00      0.00      0.00        15\n",
      "  GO:0000307       0.00      0.00      0.00        26\n",
      "  GO:0000313       0.00      0.00      0.00        85\n",
      "  GO:0000314       0.00      0.00      0.00        31\n",
      "  GO:0000315       0.00      0.00      0.00        51\n",
      "  GO:0000322       0.00      0.00      0.00       128\n",
      "  GO:0000323       0.00      0.00      0.00       352\n",
      "  GO:0000324       0.00      0.00      0.00       126\n",
      "  GO:0000325       0.00      0.00      0.00       154\n",
      "  GO:0000329       0.00      0.00      0.00        64\n",
      "  GO:0000407       0.00      0.00      0.00        17\n",
      "  GO:0000421       0.00      0.00      0.00        15\n",
      "  GO:0000428       0.00      0.00      0.00       102\n",
      "  GO:0000502       0.00      0.00      0.00        69\n",
      "  GO:0000775       0.00      0.00      0.00       153\n",
      "  GO:0000776       0.00      0.00      0.00       101\n",
      "  GO:0000779       0.00      0.00      0.00       109\n",
      "  GO:0000781       0.00      0.00      0.00        69\n",
      "  GO:0000785       0.05      0.00      0.01       466\n",
      "  GO:0000786       0.00      0.00      0.00        17\n",
      "  GO:0000791       0.00      0.00      0.00        43\n",
      "  GO:0000792       0.00      0.00      0.00        63\n",
      "  GO:0000793       0.00      0.00      0.00       174\n",
      "  GO:0000794       0.00      0.00      0.00        50\n",
      "  GO:0000795       0.00      0.00      0.00        23\n",
      "  GO:0000803       0.00      0.00      0.00        16\n",
      "  GO:0000922       0.00      0.00      0.00        55\n",
      "  GO:0000930       0.00      0.00      0.00         5\n",
      "  GO:0000932       0.00      0.00      0.00        42\n",
      "  GO:0000935       0.00      0.00      0.00        11\n",
      "  GO:0000940       0.00      0.00      0.00        23\n",
      "  GO:0000974       0.00      0.00      0.00        12\n",
      "  GO:0001411       0.00      0.00      0.00        21\n",
      "  GO:0001533       0.00      0.00      0.00        18\n",
      "  GO:0001650       0.00      0.00      0.00        67\n",
      "  GO:0001669       0.00      0.00      0.00        43\n",
      "  GO:0001673       0.00      0.00      0.00        20\n",
      "  GO:0001674       0.00      0.00      0.00        11\n",
      "  GO:0001725       0.00      0.00      0.00        23\n",
      "  GO:0001726       0.00      0.00      0.00        39\n",
      "  GO:0001750       0.00      0.00      0.00        40\n",
      "  GO:0001772       0.00      0.00      0.00        15\n",
      "  GO:0001891       0.00      0.00      0.00        17\n",
      "  GO:0001917       0.00      0.00      0.00        24\n",
      "  GO:0005575       1.00      1.00      1.00     16928\n",
      "  GO:0005576       0.00      0.00      0.00      1317\n",
      "  GO:0005581       0.00      0.00      0.00        15\n",
      "  GO:0005604       0.00      0.00      0.00        38\n",
      "  GO:0005615       0.00      0.00      0.00       982\n",
      "  GO:0005618       0.00      0.00      0.00       307\n",
      "  GO:0005622       0.80      1.00      0.89     13489\n",
      "  GO:0005628       0.00      0.00      0.00        17\n",
      "  GO:0005634       0.32      0.94      0.48      5431\n",
      "  GO:0005635       0.00      0.00      0.00       263\n",
      "  GO:0005637       0.00      0.00      0.00        15\n",
      "  GO:0005640       0.00      0.00      0.00        10\n",
      "  GO:0005643       0.00      0.00      0.00        57\n",
      "  GO:0005654       0.20      0.00      0.00      2166\n",
      "  GO:0005657       0.00      0.00      0.00        45\n",
      "  GO:0005665       0.00      0.00      0.00        18\n",
      "  GO:0005667       0.00      0.00      0.00       212\n",
      "  GO:0005669       0.00      0.00      0.00        13\n",
      "  GO:0005680       0.00      0.00      0.00        14\n",
      "  GO:0005681       0.00      0.00      0.00       115\n",
      "  GO:0005684       0.00      0.00      0.00        29\n",
      "  GO:0005685       0.00      0.00      0.00        12\n",
      "  GO:0005686       0.00      0.00      0.00         7\n",
      "  GO:0005694       0.09      0.00      0.01       842\n",
      "  GO:0005700       0.00      0.00      0.00        64\n",
      "  GO:0005703       0.00      0.00      0.00        11\n",
      "  GO:0005705       0.00      0.00      0.00        21\n",
      "  GO:0005721       0.00      0.00      0.00        24\n",
      "  GO:0005730       0.00      0.00      0.00       579\n",
      "  GO:0005732       0.00      0.00      0.00         8\n",
      "  GO:0005737       0.60      1.00      0.75     10152\n",
      "  GO:0005739       0.00      0.00      0.00      1742\n",
      "  GO:0005740       0.00      0.00      0.00       405\n",
      "  GO:0005741       0.00      0.00      0.00        99\n",
      "  GO:0005742       0.00      0.00      0.00        11\n",
      "  GO:0005743       0.00      0.00      0.00       256\n",
      "  GO:0005746       0.00      0.00      0.00        62\n",
      "  GO:0005747       0.00      0.00      0.00        35\n",
      "  GO:0005750       0.00      0.00      0.00        11\n",
      "  GO:0005751       0.00      0.00      0.00        13\n",
      "  GO:0005753       0.00      0.00      0.00        24\n",
      "  GO:0005758       0.00      0.00      0.00        42\n",
      "  GO:0005759       0.00      0.00      0.00       242\n",
      "  GO:0005761       0.00      0.00      0.00        84\n",
      "  GO:0005762       0.00      0.00      0.00        51\n",
      "  GO:0005763       0.00      0.00      0.00        31\n",
      "  GO:0005764       0.00      0.00      0.00       225\n",
      "  GO:0005765       0.00      0.00      0.00        98\n",
      "  GO:0005766       0.00      0.00      0.00        23\n",
      "  GO:0005767       0.00      0.00      0.00        17\n",
      "  GO:0005768       0.00      0.00      0.00       413\n",
      "  GO:0005769       0.00      0.00      0.00        99\n",
      "  GO:0005770       0.00      0.00      0.00        84\n",
      "  GO:0005771       0.00      0.00      0.00        24\n",
      "  GO:0005773       0.00      0.00      0.00       597\n",
      "  GO:0005774       0.00      0.00      0.00       216\n",
      "  GO:0005775       0.00      0.00      0.00        37\n",
      "  GO:0005776       0.00      0.00      0.00        44\n",
      "  GO:0005777       0.00      0.00      0.00       209\n",
      "  GO:0005778       0.00      0.00      0.00        40\n",
      "  GO:0005782       0.00      0.00      0.00        23\n",
      "  GO:0005783       0.00      0.00      0.00      1043\n",
      "  GO:0005788       0.00      0.00      0.00        81\n",
      "  GO:0005789       0.00      0.00      0.00       297\n",
      "  GO:0005790       0.00      0.00      0.00        14\n",
      "  GO:0005791       0.00      0.00      0.00        12\n",
      "  GO:0005793       0.00      0.00      0.00        20\n",
      "  GO:0005794       0.00      0.00      0.00       838\n",
      "  GO:0005795       0.00      0.00      0.00        83\n",
      "  GO:0005796       0.00      0.00      0.00        34\n",
      "  GO:0005797       0.00      0.00      0.00        22\n",
      "  GO:0005798       0.00      0.00      0.00        52\n",
      "  GO:0005801       0.00      0.00      0.00        25\n",
      "  GO:0005802       0.00      0.00      0.00       130\n",
      "  GO:0005811       0.00      0.00      0.00        74\n",
      "  GO:0005813       0.00      0.00      0.00       259\n",
      "  GO:0005814       0.00      0.00      0.00        43\n",
      "  GO:0005815       0.00      0.00      0.00       406\n",
      "  GO:0005816       0.00      0.00      0.00        52\n",
      "  GO:0005818       0.00      0.00      0.00        22\n",
      "  GO:0005819       0.00      0.00      0.00       203\n",
      "  GO:0005826       0.00      0.00      0.00        20\n",
      "  GO:0005829       0.00      0.00      0.00      3485\n",
      "  GO:0005832       0.00      0.00      0.00        10\n",
      "  GO:0005834       0.00      0.00      0.00        18\n",
      "  GO:0005838       0.00      0.00      0.00        19\n",
      "  GO:0005839       0.00      0.00      0.00        23\n",
      "  GO:0005840       0.00      0.00      0.00       284\n",
      "  GO:0005844       0.00      0.00      0.00        53\n",
      "  GO:0005847       0.00      0.00      0.00        14\n",
      "  GO:0005849       0.00      0.00      0.00        16\n",
      "  GO:0005852       0.00      0.00      0.00        12\n",
      "  GO:0005856       0.00      0.00      0.00      1100\n",
      "  GO:0005865       0.00      0.00      0.00        16\n",
      "  GO:0005871       0.00      0.00      0.00        15\n",
      "  GO:0005874       0.00      0.00      0.00       152\n",
      "  GO:0005875       0.00      0.00      0.00        67\n",
      "  GO:0005876       0.00      0.00      0.00        51\n",
      "  GO:0005879       0.00      0.00      0.00        19\n",
      "  GO:0005881       0.00      0.00      0.00        67\n",
      "  GO:0005882       0.00      0.00      0.00        25\n",
      "  GO:0005884       0.00      0.00      0.00        40\n",
      "  GO:0005885       0.00      0.00      0.00        12\n",
      "  GO:0005886       0.00      0.00      0.00      3153\n",
      "  GO:0005891       0.00      0.00      0.00        13\n",
      "  GO:0005892       0.00      0.00      0.00        12\n",
      "  GO:0005901       0.00      0.00      0.00        34\n",
      "  GO:0005902       0.00      0.00      0.00        31\n",
      "  GO:0005903       0.00      0.00      0.00        48\n",
      "  GO:0005905       0.00      0.00      0.00        24\n",
      "  GO:0005911       0.00      0.00      0.00       396\n",
      "  GO:0005912       0.00      0.00      0.00        65\n",
      "  GO:0005918       0.00      0.00      0.00        13\n",
      "  GO:0005921       0.00      0.00      0.00        18\n",
      "  GO:0005923       0.00      0.00      0.00        31\n",
      "  GO:0005925       0.00      0.00      0.00       116\n",
      "  GO:0005927       0.00      0.00      0.00         5\n",
      "  GO:0005929       0.00      0.00      0.00       539\n",
      "  GO:0005930       0.00      0.00      0.00        98\n",
      "  GO:0005933       0.00      0.00      0.00        55\n",
      "  GO:0005934       0.00      0.00      0.00        22\n",
      "  GO:0005935       0.00      0.00      0.00        41\n",
      "  GO:0005937       0.00      0.00      0.00        29\n",
      "  GO:0005938       0.00      0.00      0.00       278\n",
      "  GO:0005942       0.00      0.00      0.00        15\n",
      "  GO:0008021       0.00      0.00      0.00        88\n",
      "  GO:0008023       0.00      0.00      0.00        34\n",
      "  GO:0008076       0.00      0.00      0.00        25\n",
      "  GO:0008180       0.00      0.00      0.00        19\n",
      "  GO:0008287       0.00      0.00      0.00        24\n",
      "  GO:0008305       0.00      0.00      0.00        12\n",
      "  GO:0008328       0.00      0.00      0.00        14\n",
      "  GO:0009274       0.00      0.00      0.00       165\n",
      "  GO:0009277       0.00      0.00      0.00        58\n",
      "  GO:0009279       0.00      0.00      0.00        38\n",
      "  GO:0009295       0.00      0.00      0.00        43\n",
      "  GO:0009505       0.00      0.00      0.00        75\n",
      "  GO:0009506       0.00      0.00      0.00       172\n",
      "  GO:0009507       0.00      0.00      0.00       418\n",
      "  GO:0009524       0.00      0.00      0.00        15\n",
      "  GO:0009526       0.00      0.00      0.00       160\n",
      "  GO:0009528       0.00      0.00      0.00        14\n",
      "  GO:0009532       0.00      0.00      0.00       121\n",
      "  GO:0009534       0.00      0.00      0.00        92\n",
      "  GO:0009535       0.00      0.00      0.00        70\n",
      "  GO:0009536       0.00      0.00      0.00       663\n",
      "  GO:0009570       0.00      0.00      0.00       121\n",
      "  GO:0009579       0.00      0.00      0.00       117\n",
      "  GO:0009705       0.00      0.00      0.00        23\n",
      "  GO:0009706       0.00      0.00      0.00        12\n",
      "  GO:0009897       0.00      0.00      0.00       116\n",
      "  GO:0009898       0.00      0.00      0.00        77\n",
      "  GO:0009925       0.00      0.00      0.00       127\n",
      "  GO:0009941       0.00      0.00      0.00       109\n",
      "  GO:0009986       0.00      0.00      0.00       285\n",
      "  GO:0010008       0.00      0.00      0.00       103\n",
      "  GO:0010287       0.00      0.00      0.00         7\n",
      "  GO:0010369       0.00      0.00      0.00         9\n",
      "  GO:0010494       0.00      0.00      0.00        53\n",
      "  GO:0012505       0.00      0.00      0.00      2473\n",
      "  GO:0012506       0.00      0.00      0.00       309\n",
      "  GO:0012507       0.00      0.00      0.00        10\n",
      "  GO:0014069       0.00      0.00      0.00        89\n",
      "  GO:0014704       0.00      0.00      0.00        19\n",
      "  GO:0015030       0.00      0.00      0.00        11\n",
      "  GO:0015629       0.00      0.00      0.00       239\n",
      "  GO:0015630       0.00      0.00      0.00       687\n",
      "  GO:0015934       0.00      0.00      0.00       143\n",
      "  GO:0015935       0.00      0.00      0.00       106\n",
      "  GO:0016010       0.00      0.00      0.00         9\n",
      "  GO:0016020       0.33      0.01      0.02      4692\n",
      "  GO:0016028       0.00      0.00      0.00        15\n",
      "  GO:0016234       0.00      0.00      0.00        28\n",
      "  GO:0016235       0.00      0.00      0.00        11\n",
      "  GO:0016323       0.00      0.00      0.00       111\n",
      "  GO:0016324       0.00      0.00      0.00       168\n",
      "  GO:0016327       0.00      0.00      0.00        10\n",
      "  GO:0016328       0.00      0.00      0.00        28\n",
      "  GO:0016363       0.00      0.00      0.00        21\n",
      "  GO:0016459       0.00      0.00      0.00        16\n",
      "  GO:0016469       0.00      0.00      0.00        48\n",
      "  GO:0016471       0.00      0.00      0.00        12\n",
      "  GO:0016514       0.00      0.00      0.00        18\n",
      "  GO:0016528       0.00      0.00      0.00        30\n",
      "  GO:0016529       0.00      0.00      0.00        24\n",
      "  GO:0016581       0.00      0.00      0.00        10\n",
      "  GO:0016591       0.00      0.00      0.00        64\n",
      "  GO:0016592       0.00      0.00      0.00        27\n",
      "  GO:0016604       0.00      0.00      0.00       313\n",
      "  GO:0016605       0.00      0.00      0.00        21\n",
      "  GO:0016607       0.00      0.00      0.00       128\n",
      "  GO:0017053       0.00      0.00      0.00        30\n",
      "  GO:0018995       0.00      0.00      0.00       161\n",
      "  GO:0019005       0.00      0.00      0.00        27\n",
      "  GO:0019028       0.00      0.00      0.00        16\n",
      "  GO:0019866       0.00      0.00      0.00       286\n",
      "  GO:0019867       0.00      0.00      0.00       224\n",
      "  GO:0019897       0.00      0.00      0.00        46\n",
      "  GO:0019898       0.00      0.00      0.00        83\n",
      "  GO:0020002       0.00      0.00      0.00        32\n",
      "  GO:0020003       0.00      0.00      0.00        19\n",
      "  GO:0020007       0.00      0.00      0.00         8\n",
      "  GO:0020011       0.00      0.00      0.00        19\n",
      "  GO:0020015       0.00      0.00      0.00        38\n",
      "  GO:0020016       0.00      0.00      0.00        17\n",
      "  GO:0020023       0.00      0.00      0.00        60\n",
      "  GO:0020036       0.00      0.00      0.00        10\n",
      "  GO:0022624       0.00      0.00      0.00        24\n",
      "  GO:0022625       0.00      0.00      0.00        90\n",
      "  GO:0022626       0.00      0.00      0.00       182\n",
      "  GO:0022627       0.00      0.00      0.00        74\n",
      "  GO:0030014       0.00      0.00      0.00        11\n",
      "  GO:0030016       0.00      0.00      0.00       121\n",
      "  GO:0030017       0.00      0.00      0.00       100\n",
      "  GO:0030018       0.00      0.00      0.00        43\n",
      "  GO:0030027       0.00      0.00      0.00        45\n",
      "  GO:0030054       0.00      0.00      0.00      1158\n",
      "  GO:0030055       0.00      0.00      0.00       129\n",
      "  GO:0030117       0.00      0.00      0.00        44\n",
      "  GO:0030118       0.00      0.00      0.00        23\n",
      "  GO:0030119       0.00      0.00      0.00        19\n",
      "  GO:0030120       0.00      0.00      0.00        34\n",
      "  GO:0030125       0.00      0.00      0.00        18\n",
      "  GO:0030131       0.00      0.00      0.00        15\n",
      "  GO:0030133       0.00      0.00      0.00       151\n",
      "  GO:0030134       0.00      0.00      0.00        22\n",
      "  GO:0030135       0.00      0.00      0.00       117\n",
      "  GO:0030136       0.00      0.00      0.00        73\n",
      "  GO:0030137       0.00      0.00      0.00        23\n",
      "  GO:0030139       0.00      0.00      0.00       167\n",
      "  GO:0030140       0.00      0.00      0.00        21\n",
      "  GO:0030141       0.00      0.00      0.00       212\n",
      "  GO:0030175       0.00      0.00      0.00        43\n",
      "  GO:0030286       0.00      0.00      0.00        19\n",
      "  GO:0030288       0.00      0.00      0.00        38\n",
      "  GO:0030312       0.00      0.00      0.00       646\n",
      "  GO:0030313       0.00      0.00      0.00        82\n",
      "  GO:0030315       0.00      0.00      0.00        23\n",
      "  GO:0030424       0.00      0.00      0.00       318\n",
      "  GO:0030425       0.00      0.00      0.00       235\n",
      "  GO:0030426       0.00      0.00      0.00        63\n",
      "  GO:0030427       0.00      0.00      0.00       151\n",
      "  GO:0030428       0.00      0.00      0.00        17\n",
      "  GO:0030430       0.00      0.00      0.00        78\n",
      "  GO:0030445       0.00      0.00      0.00         9\n",
      "  GO:0030446       0.00      0.00      0.00        13\n",
      "  GO:0030479       0.00      0.00      0.00        18\n",
      "  GO:0030496       0.00      0.00      0.00        61\n",
      "  GO:0030532       0.00      0.00      0.00        36\n",
      "  GO:0030658       0.00      0.00      0.00        65\n",
      "  GO:0030659       0.00      0.00      0.00       298\n",
      "  GO:0030660       0.00      0.00      0.00        27\n",
      "  GO:0030662       0.00      0.00      0.00        61\n",
      "  GO:0030665       0.00      0.00      0.00        39\n",
      "  GO:0030666       0.00      0.00      0.00        48\n",
      "  GO:0030667       0.00      0.00      0.00        66\n",
      "  GO:0030669       0.00      0.00      0.00        21\n",
      "  GO:0030670       0.00      0.00      0.00        17\n",
      "  GO:0030672       0.00      0.00      0.00        42\n",
      "  GO:0030684       0.00      0.00      0.00        53\n",
      "  GO:0030686       0.00      0.00      0.00        23\n",
      "  GO:0030687       0.00      0.00      0.00         7\n",
      "  GO:0030863       0.00      0.00      0.00       102\n",
      "  GO:0030864       0.00      0.00      0.00        74\n",
      "  GO:0030867       0.00      0.00      0.00         6\n",
      "  GO:0030880       0.00      0.00      0.00       102\n",
      "  GO:0030894       0.00      0.00      0.00        22\n",
      "  GO:0030964       0.00      0.00      0.00        46\n",
      "  GO:0030981       0.00      0.00      0.00        14\n",
      "  GO:0030990       0.00      0.00      0.00        13\n",
      "  GO:0030992       0.00      0.00      0.00        11\n",
      "  GO:0031010       0.00      0.00      0.00         5\n",
      "  GO:0031011       0.00      0.00      0.00        18\n",
      "  GO:0031012       0.00      0.00      0.00       301\n",
      "  GO:0031045       0.00      0.00      0.00        17\n",
      "  GO:0031082       0.00      0.00      0.00        14\n",
      "  GO:0031090       0.00      0.00      0.00      1433\n",
      "  GO:0031091       0.00      0.00      0.00        12\n",
      "  GO:0031093       0.00      0.00      0.00         9\n",
      "  GO:0031097       0.00      0.00      0.00         7\n",
      "  GO:0031143       0.00      0.00      0.00        14\n",
      "  GO:0031160       0.00      0.00      0.00        10\n",
      "  GO:0031201       0.00      0.00      0.00        21\n",
      "  GO:0031234       0.00      0.00      0.00        24\n",
      "  GO:0031248       0.00      0.00      0.00        73\n",
      "  GO:0031252       0.00      0.00      0.00       122\n",
      "  GO:0031253       0.00      0.00      0.00       104\n",
      "  GO:0031256       0.00      0.00      0.00        44\n",
      "  GO:0031261       0.00      0.00      0.00        23\n",
      "  GO:0031312       0.00      0.00      0.00        12\n",
      "  GO:0031410       0.00      0.00      0.00       996\n",
      "  GO:0031430       0.00      0.00      0.00        13\n",
      "  GO:0031461       0.00      0.00      0.00        81\n",
      "  GO:0031463       0.00      0.00      0.00        14\n",
      "  GO:0031514       0.00      0.00      0.00        80\n",
      "  GO:0031519       0.00      0.00      0.00        19\n",
      "  GO:0031526       0.00      0.00      0.00        25\n",
      "  GO:0031594       0.00      0.00      0.00        70\n",
      "  GO:0031672       0.00      0.00      0.00        23\n",
      "  GO:0031674       0.00      0.00      0.00        50\n",
      "  GO:0031901       0.00      0.00      0.00        14\n",
      "  GO:0031902       0.00      0.00      0.00        22\n",
      "  GO:0031903       0.00      0.00      0.00        40\n",
      "  GO:0031907       0.00      0.00      0.00        23\n",
      "  GO:0031941       0.00      0.00      0.00         8\n",
      "  GO:0031965       0.00      0.00      0.00        95\n",
      "  GO:0031966       0.00      0.00      0.00       365\n",
      "  GO:0031967       0.00      0.00      0.00       824\n",
      "  GO:0031968       0.00      0.00      0.00       183\n",
      "  GO:0031969       0.00      0.00      0.00        21\n",
      "  GO:0031970       0.00      0.00      0.00        43\n",
      "  GO:0031974       0.21      0.00      0.00      3156\n",
      "  GO:0031975       0.00      0.00      0.00       906\n",
      "  GO:0031976       0.00      0.00      0.00        93\n",
      "  GO:0031977       0.00      0.00      0.00        15\n",
      "  GO:0031981       0.23      0.00      0.00      2712\n",
      "  GO:0031982       0.00      0.00      0.00      1347\n",
      "  GO:0031983       0.00      0.00      0.00        64\n",
      "  GO:0031984       0.00      0.00      0.00       488\n",
      "  GO:0031985       0.00      0.00      0.00        72\n",
      "  GO:0032010       0.00      0.00      0.00        16\n",
      "  GO:0032040       0.00      0.00      0.00        39\n",
      "  GO:0032153       0.00      0.00      0.00       120\n",
      "  GO:0032154       0.00      0.00      0.00        44\n",
      "  GO:0032156       0.00      0.00      0.00        14\n",
      "  GO:0032279       0.00      0.00      0.00        95\n",
      "  GO:0032391       0.00      0.00      0.00        10\n",
      "  GO:0032420       0.00      0.00      0.00        16\n",
      "  GO:0032421       0.00      0.00      0.00        16\n",
      "  GO:0032432       0.00      0.00      0.00        31\n",
      "  GO:0032541       0.00      0.00      0.00         9\n",
      "  GO:0032587       0.00      0.00      0.00        17\n",
      "  GO:0032588       0.00      0.00      0.00        25\n",
      "  GO:0032589       0.00      0.00      0.00        23\n",
      "  GO:0032590       0.00      0.00      0.00        12\n",
      "  GO:0032806       0.00      0.00      0.00        22\n",
      "  GO:0032809       0.00      0.00      0.00        10\n",
      "  GO:0032838       0.00      0.00      0.00       243\n",
      "  GO:0032991       0.23      0.01      0.01      3265\n",
      "  GO:0032993       0.05      0.00      0.01       533\n",
      "  GO:0033176       0.00      0.00      0.00        19\n",
      "  GO:0033178       0.00      0.00      0.00         9\n",
      "  GO:0033643       0.00      0.00      0.00       157\n",
      "  GO:0033644       0.00      0.00      0.00        46\n",
      "  GO:0033646       0.00      0.00      0.00       104\n",
      "  GO:0033647       0.00      0.00      0.00        45\n",
      "  GO:0033648       0.00      0.00      0.00        43\n",
      "  GO:0033655       0.00      0.00      0.00        53\n",
      "  GO:0034357       0.00      0.00      0.00        81\n",
      "  GO:0034399       0.00      0.00      0.00        75\n",
      "  GO:0034451       0.00      0.00      0.00        45\n",
      "  GO:0034518       0.00      0.00      0.00        16\n",
      "  GO:0034702       0.00      0.00      0.00        91\n",
      "  GO:0034703       0.00      0.00      0.00        59\n",
      "  GO:0034704       0.00      0.00      0.00        19\n",
      "  GO:0034705       0.00      0.00      0.00        29\n",
      "  GO:0034708       0.00      0.00      0.00        43\n",
      "  GO:0034774       0.00      0.00      0.00        63\n",
      "  GO:0035097       0.00      0.00      0.00        27\n",
      "  GO:0035267       0.00      0.00      0.00        12\n",
      "  GO:0035371       0.00      0.00      0.00        15\n",
      "  GO:0035577       0.00      0.00      0.00         7\n",
      "  GO:0035578       0.00      0.00      0.00        17\n",
      "  GO:0035579       0.00      0.00      0.00        21\n",
      "  GO:0035580       0.00      0.00      0.00        14\n",
      "  GO:0035770       0.00      0.00      0.00       141\n",
      "  GO:0035838       0.00      0.00      0.00        12\n",
      "  GO:0035861       0.00      0.00      0.00        37\n",
      "  GO:0035869       0.00      0.00      0.00        45\n",
      "  GO:0036064       0.00      0.00      0.00        84\n",
      "  GO:0036126       0.00      0.00      0.00        51\n",
      "  GO:0036379       0.00      0.00      0.00        21\n",
      "  GO:0036452       0.00      0.00      0.00         8\n",
      "  GO:0036464       0.00      0.00      0.00       131\n",
      "  GO:0036477       0.00      0.00      0.00       403\n",
      "  GO:0042025       0.00      0.00      0.00        31\n",
      "  GO:0042170       0.00      0.00      0.00        94\n",
      "  GO:0042175       0.00      0.00      0.00       311\n",
      "  GO:0042383       0.00      0.00      0.00        60\n",
      "  GO:0042470       0.00      0.00      0.00         7\n",
      "  GO:0042555       0.00      0.00      0.00        15\n",
      "  GO:0042575       0.00      0.00      0.00        22\n",
      "  GO:0042579       0.00      0.00      0.00       209\n",
      "  GO:0042581       0.00      0.00      0.00        36\n",
      "  GO:0042582       0.00      0.00      0.00        23\n",
      "  GO:0042597       0.00      0.00      0.00        45\n",
      "  GO:0042641       0.00      0.00      0.00        26\n",
      "  GO:0042644       0.00      0.00      0.00        11\n",
      "  GO:0042645       0.00      0.00      0.00        26\n",
      "  GO:0042646       0.00      0.00      0.00        11\n",
      "  GO:0042651       0.00      0.00      0.00        77\n",
      "  GO:0042734       0.00      0.00      0.00        68\n",
      "  GO:0042763       0.00      0.00      0.00        18\n",
      "  GO:0042764       0.00      0.00      0.00        17\n",
      "  GO:0042788       0.00      0.00      0.00        32\n",
      "  GO:0042995       0.00      0.00      0.00      1264\n",
      "  GO:0043005       0.00      0.00      0.00       574\n",
      "  GO:0043025       0.00      0.00      0.00       279\n",
      "  GO:0043073       0.00      0.00      0.00        35\n",
      "  GO:0043186       0.00      0.00      0.00        28\n",
      "  GO:0043189       0.00      0.00      0.00        12\n",
      "  GO:0043190       0.00      0.00      0.00        19\n",
      "  GO:0043194       0.00      0.00      0.00         9\n",
      "  GO:0043195       0.00      0.00      0.00        43\n",
      "  GO:0043197       0.00      0.00      0.00        46\n",
      "  GO:0043198       0.00      0.00      0.00        15\n",
      "  GO:0043202       0.00      0.00      0.00        18\n",
      "  GO:0043204       0.00      0.00      0.00        45\n",
      "  GO:0043209       0.00      0.00      0.00        37\n",
      "  GO:0043226       0.68      1.00      0.81     11592\n",
      "  GO:0043227       0.62      1.00      0.77     10557\n",
      "  GO:0043228       0.25      0.01      0.01      3058\n",
      "  GO:0043229       0.66      1.00      0.79     11136\n",
      "  GO:0043230       0.00      0.00      0.00       431\n",
      "  GO:0043231       0.59      1.00      0.74      9992\n",
      "  GO:0043232       0.25      0.01      0.01      3053\n",
      "  GO:0043233       0.21      0.00      0.00      3156\n",
      "  GO:0043235       0.00      0.00      0.00       105\n",
      "  GO:0043292       0.00      0.00      0.00       145\n",
      "  GO:0043296       0.00      0.00      0.00        65\n",
      "  GO:0043332       0.00      0.00      0.00        25\n",
      "  GO:0043596       0.00      0.00      0.00        32\n",
      "  GO:0043601       0.00      0.00      0.00        17\n",
      "  GO:0043656       0.00      0.00      0.00       104\n",
      "  GO:0043657       0.00      0.00      0.00       159\n",
      "  GO:0043679       0.00      0.00      0.00        80\n",
      "  GO:0044232       0.00      0.00      0.00        23\n",
      "  GO:0044291       0.00      0.00      0.00        23\n",
      "  GO:0044295       0.00      0.00      0.00        14\n",
      "  GO:0044297       0.00      0.00      0.00       307\n",
      "  GO:0044298       0.00      0.00      0.00        14\n",
      "  GO:0044304       0.00      0.00      0.00        33\n",
      "  GO:0044306       0.00      0.00      0.00        83\n",
      "  GO:0044309       0.00      0.00      0.00        47\n",
      "  GO:0044391       0.00      0.00      0.00       248\n",
      "  GO:0044423       0.00      0.00      0.00        30\n",
      "  GO:0044732       0.00      0.00      0.00        30\n",
      "  GO:0044853       0.00      0.00      0.00        44\n",
      "  GO:0045111       0.00      0.00      0.00        49\n",
      "  GO:0045121       0.00      0.00      0.00       104\n",
      "  GO:0045169       0.00      0.00      0.00        21\n",
      "  GO:0045171       0.00      0.00      0.00        42\n",
      "  GO:0045172       0.00      0.00      0.00         9\n",
      "  GO:0045177       0.00      0.00      0.00       222\n",
      "  GO:0045178       0.00      0.00      0.00       138\n",
      "  GO:0045179       0.00      0.00      0.00        26\n",
      "  GO:0045202       0.00      0.00      0.00       592\n",
      "  GO:0045211       0.00      0.00      0.00        89\n",
      "  GO:0045239       0.00      0.00      0.00         7\n",
      "  GO:0045259       0.00      0.00      0.00        28\n",
      "  GO:0045271       0.00      0.00      0.00        39\n",
      "  GO:0045273       0.00      0.00      0.00         4\n",
      "  GO:0045275       0.00      0.00      0.00        11\n",
      "  GO:0045277       0.00      0.00      0.00        16\n",
      "  GO:0045293       0.00      0.00      0.00        15\n",
      "  GO:0045334       0.00      0.00      0.00        23\n",
      "  GO:0045335       0.00      0.00      0.00       111\n",
      "  GO:0045495       0.00      0.00      0.00        32\n",
      "  GO:0046540       0.00      0.00      0.00        11\n",
      "  GO:0048046       0.00      0.00      0.00        44\n",
      "  GO:0048471       0.00      0.00      0.00       225\n",
      "  GO:0048475       0.00      0.00      0.00        44\n",
      "  GO:0048770       0.00      0.00      0.00         7\n",
      "  GO:0048786       0.00      0.00      0.00        43\n",
      "  GO:0048787       0.00      0.00      0.00        14\n",
      "  GO:0051233       0.00      0.00      0.00        27\n",
      "  GO:0051285       0.00      0.00      0.00        10\n",
      "  GO:0051286       0.00      0.00      0.00       105\n",
      "  GO:0055028       0.00      0.00      0.00        13\n",
      "  GO:0055029       0.00      0.00      0.00        84\n",
      "  GO:0055035       0.00      0.00      0.00        71\n",
      "  GO:0055037       0.00      0.00      0.00        69\n",
      "  GO:0055038       0.00      0.00      0.00        18\n",
      "  GO:0055044       0.00      0.00      0.00       172\n",
      "  GO:0055120       0.00      0.00      0.00        29\n",
      "  GO:0060076       0.00      0.00      0.00        34\n",
      "  GO:0060170       0.00      0.00      0.00        26\n",
      "  GO:0060187       0.00      0.00      0.00       113\n",
      "  GO:0060205       0.00      0.00      0.00        63\n",
      "  GO:0060293       0.00      0.00      0.00        29\n",
      "  GO:0061174       0.00      0.00      0.00        14\n",
      "  GO:0061645       0.00      0.00      0.00        20\n",
      "  GO:0061695       0.00      0.00      0.00       219\n",
      "  GO:0062023       0.00      0.00      0.00       173\n",
      "  GO:0062039       0.00      0.00      0.00        66\n",
      "  GO:0062040       0.00      0.00      0.00        66\n",
      "  GO:0065010       0.00      0.00      0.00       431\n",
      "  GO:0070013       0.21      0.00      0.00      3156\n",
      "  GO:0070062       0.00      0.00      0.00       392\n",
      "  GO:0070069       0.00      0.00      0.00        32\n",
      "  GO:0070160       0.00      0.00      0.00        52\n",
      "  GO:0070161       0.00      0.00      0.00       520\n",
      "  GO:0070382       0.00      0.00      0.00       100\n",
      "  GO:0070461       0.00      0.00      0.00        35\n",
      "  GO:0070469       0.00      0.00      0.00        72\n",
      "  GO:0070603       0.00      0.00      0.00        72\n",
      "  GO:0070820       0.00      0.00      0.00        29\n",
      "  GO:0070821       0.00      0.00      0.00        17\n",
      "  GO:0070822       0.00      0.00      0.00         7\n",
      "  GO:0070847       0.00      0.00      0.00         7\n",
      "  GO:0070938       0.00      0.00      0.00        31\n",
      "  GO:0070971       0.00      0.00      0.00        11\n",
      "  GO:0071011       0.00      0.00      0.00        69\n",
      "  GO:0071013       0.00      0.00      0.00        58\n",
      "  GO:0071014       0.00      0.00      0.00        10\n",
      "  GO:0071162       0.00      0.00      0.00        15\n",
      "  GO:0071782       0.00      0.00      0.00        14\n",
      "  GO:0071944       0.00      0.00      0.00      3846\n",
      "  GO:0072562       0.00      0.00      0.00        27\n",
      "  GO:0072686       0.00      0.00      0.00        96\n",
      "  GO:0072687       0.00      0.00      0.00        12\n",
      "  GO:0080008       0.00      0.00      0.00        13\n",
      "  GO:0090406       0.00      0.00      0.00        12\n",
      "  GO:0090545       0.00      0.00      0.00        10\n",
      "  GO:0090571       0.00      0.00      0.00         9\n",
      "  GO:0090575       0.00      0.00      0.00       107\n",
      "  GO:0090665       0.00      0.00      0.00        10\n",
      "  GO:0090734       0.00      0.00      0.00        43\n",
      "  GO:0097014       0.00      0.00      0.00       226\n",
      "  GO:0097038       0.00      0.00      0.00         9\n",
      "  GO:0097060       0.00      0.00      0.00       139\n",
      "  GO:0097225       0.00      0.00      0.00        11\n",
      "  GO:0097346       0.00      0.00      0.00        25\n",
      "  GO:0097386       0.00      0.00      0.00        10\n",
      "  GO:0097447       0.00      0.00      0.00       236\n",
      "  GO:0097517       0.00      0.00      0.00        23\n",
      "  GO:0097525       0.00      0.00      0.00        32\n",
      "  GO:0097526       0.00      0.00      0.00        11\n",
      "  GO:0097542       0.00      0.00      0.00        16\n",
      "  GO:0097546       0.00      0.00      0.00        23\n",
      "  GO:0097708       0.00      0.00      0.00      1001\n",
      "  GO:0097729       0.00      0.00      0.00        59\n",
      "  GO:0097730       0.00      0.00      0.00        83\n",
      "  GO:0097731       0.00      0.00      0.00        49\n",
      "  GO:0097733       0.00      0.00      0.00        48\n",
      "  GO:0098533       0.00      0.00      0.00        24\n",
      "  GO:0098552       0.00      0.00      0.00       222\n",
      "  GO:0098554       0.00      0.00      0.00         7\n",
      "  GO:0098562       0.00      0.00      0.00        97\n",
      "  GO:0098576       0.00      0.00      0.00         9\n",
      "  GO:0098588       0.00      0.00      0.00       808\n",
      "  GO:0098590       0.00      0.00      0.00       567\n",
      "  GO:0098636       0.00      0.00      0.00        19\n",
      "  GO:0098644       0.00      0.00      0.00        10\n",
      "  GO:0098685       0.00      0.00      0.00        25\n",
      "  GO:0098686       0.00      0.00      0.00        15\n",
      "  GO:0098687       0.00      0.00      0.00       244\n",
      "  GO:0098688       0.00      0.00      0.00        13\n",
      "  GO:0098791       0.00      0.00      0.00       182\n",
      "  GO:0098793       0.00      0.00      0.00       263\n",
      "  GO:0098794       0.00      0.00      0.00       222\n",
      "  GO:0098796       0.00      0.00      0.00       587\n",
      "  GO:0098797       0.00      0.00      0.00       254\n",
      "  GO:0098798       0.00      0.00      0.00       230\n",
      "  GO:0098799       0.00      0.00      0.00        13\n",
      "  GO:0098800       0.00      0.00      0.00       106\n",
      "  GO:0098802       0.00      0.00      0.00        59\n",
      "  GO:0098803       0.00      0.00      0.00        69\n",
      "  GO:0098827       0.00      0.00      0.00       313\n",
      "  GO:0098839       0.00      0.00      0.00        26\n",
      "  GO:0098852       0.00      0.00      0.00       162\n",
      "  GO:0098857       0.00      0.00      0.00       104\n",
      "  GO:0098858       0.00      0.00      0.00        82\n",
      "  GO:0098862       0.00      0.00      0.00        62\n",
      "  GO:0098878       0.00      0.00      0.00        14\n",
      "  GO:0098978       0.00      0.00      0.00       184\n",
      "  GO:0098982       0.00      0.00      0.00        29\n",
      "  GO:0098984       0.00      0.00      0.00       106\n",
      "  GO:0099023       0.00      0.00      0.00        28\n",
      "  GO:0099080       0.00      0.00      0.00       601\n",
      "  GO:0099081       0.00      0.00      0.00       374\n",
      "  GO:0099086       0.00      0.00      0.00        26\n",
      "  GO:0099091       0.00      0.00      0.00        14\n",
      "  GO:0099092       0.00      0.00      0.00        12\n",
      "  GO:0099501       0.00      0.00      0.00        42\n",
      "  GO:0099503       0.00      0.00      0.00       337\n",
      "  GO:0099512       0.00      0.00      0.00       372\n",
      "  GO:0099513       0.00      0.00      0.00       218\n",
      "  GO:0099522       0.00      0.00      0.00        11\n",
      "  GO:0099568       0.00      0.00      0.00       296\n",
      "  GO:0099572       0.00      0.00      0.00       110\n",
      "  GO:0099634       0.00      0.00      0.00        36\n",
      "  GO:0099738       0.00      0.00      0.00        54\n",
      "  GO:0101002       0.00      0.00      0.00        33\n",
      "  GO:0101003       0.00      0.00      0.00        10\n",
      "  GO:0101031       0.00      0.00      0.00        23\n",
      "  GO:0110085       0.00      0.00      0.00        13\n",
      "  GO:0110165       0.99      1.00      1.00     16782\n",
      "  GO:0120025       0.00      0.00      0.00      1240\n",
      "  GO:0120111       0.00      0.00      0.00        19\n",
      "  GO:0120114       0.00      0.00      0.00        50\n",
      "  GO:0120119       0.00      0.00      0.00        13\n",
      "  GO:0140220       0.00      0.00      0.00         6\n",
      "  GO:0140445       0.00      0.00      0.00        17\n",
      "  GO:0140513       0.00      0.00      0.00       769\n",
      "  GO:0140534       0.00      0.00      0.00        57\n",
      "  GO:0140535       0.00      0.00      0.00       598\n",
      "  GO:0150034       0.00      0.00      0.00       139\n",
      "  GO:1902493       0.00      0.00      0.00        74\n",
      "  GO:1902494       0.02      0.00      0.00      1113\n",
      "  GO:1902495       0.00      0.00      0.00       178\n",
      "  GO:1902554       0.00      0.00      0.00        75\n",
      "  GO:1902555       0.00      0.00      0.00        13\n",
      "  GO:1902562       0.00      0.00      0.00        25\n",
      "  GO:1902911       0.00      0.00      0.00        88\n",
      "  GO:1903293       0.00      0.00      0.00        26\n",
      "  GO:1903561       0.00      0.00      0.00       410\n",
      "  GO:1904724       0.00      0.00      0.00         9\n",
      "  GO:1904813       0.00      0.00      0.00        23\n",
      "  GO:1904949       0.00      0.00      0.00       121\n",
      "  GO:1905348       0.00      0.00      0.00        22\n",
      "  GO:1905354       0.00      0.00      0.00        11\n",
      "  GO:1905360       0.00      0.00      0.00        22\n",
      "  GO:1905368       0.00      0.00      0.00       109\n",
      "  GO:1905369       0.00      0.00      0.00        77\n",
      "  GO:1990023       0.00      0.00      0.00        16\n",
      "  GO:1990204       0.00      0.00      0.00        96\n",
      "  GO:1990234       0.02      0.00      0.00       511\n",
      "  GO:1990351       0.00      0.00      0.00       197\n",
      "  GO:1990752       0.00      0.00      0.00        19\n",
      "  GO:1990904       0.00      0.00      0.00       539\n",
      "\n",
      "   micro avg       0.69      0.48      0.57    221564\n",
      "   macro avg       0.01      0.01      0.01    221564\n",
      "weighted avg       0.39      0.48      0.41    221564\n",
      " samples avg       0.70      0.55      0.58    221564\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, model.predict(X_test_scaled) > 0.3, target_names=mlb.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "1/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "2/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "3/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "4/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "5/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "6/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "7/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "8/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "9/15 F1-Score 0.010\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "10/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "11/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n",
      "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "12/15 F1-Score 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eliat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 51\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_f1\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Load your data (X_array, Y_array)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# X_array: Features, Y_array: Multi-label targets\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Example: X_array.shape = (n_samples, n_features), Y_array.shape = (n_samples, n_classes)\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m results_f1 \u001b[38;5;241m=\u001b[39m evaluate_model(X_array, Y_array)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Summarize performance\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1-Score: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (mean(results_f1), std(results_f1)))\n",
      "Cell \u001b[1;32mIn[14], line 33\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(n_inputs, n_outputs)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Make a prediction\u001b[39;00m\n\u001b[0;32m     35\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import mean, std\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Evaluate the model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    "    results_f1 = []\n",
    "    n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    "    n_splits, n_repeats = 5, 3\n",
    "    cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "    estimate = n_splits * n_repeats\n",
    "    i = 0\n",
    "    \n",
    "    for train_ix, test_ix in cv.split(X):\n",
    "        i += 1\n",
    "        # Prepare data\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        # Define model\n",
    "        model = get_model(n_inputs, n_outputs)\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train, verbose=5, epochs=10)\n",
    "        # Make a prediction\n",
    "        yhat = model.predict(X_test)\n",
    "        \n",
    "        # Ensure probabilities are within (0, 1.000]\n",
    "        yhat = np.clip(yhat, 1e-6, 1.0)\n",
    "        \n",
    "        # Calculate F1-score (macro)\n",
    "        f1 = f1_score(y_test, (yhat > 0.5).astype(int), average='macro')\n",
    "        print(f'{i}/{estimate} F1-Score {f1:.3f}')\n",
    "        results_f1.append(f1)\n",
    "    \n",
    "    return results_f1\n",
    "\n",
    "# Load your data (X_array, Y_array)\n",
    "# X_array: Features, Y_array: Multi-label targets\n",
    "# Example: X_array.shape = (n_samples, n_features), Y_array.shape = (n_samples, n_classes)\n",
    "\n",
    "results_f1 = evaluate_model(X_array, Y_array)\n",
    "\n",
    "# Summarize performance\n",
    "print('F1-Score: %.3f (%.3f)' % (mean(results_f1), std(results_f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
