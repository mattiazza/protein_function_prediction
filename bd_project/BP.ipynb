{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score, auc\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from load_data import load_dataset, load_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ontology = 'biological_process'\n",
    "\n",
    "\n",
    "n_test = ''\n",
    "\n",
    "\n",
    "training_data_path = Path('../data/train')\n",
    "test_data_path = Path('../data/test')\n",
    "baseline_data_path = Path('../data/baseline')\n",
    "datasets_path = Path('../data/datasets')\n",
    "models_path = Path('../data/models')\n",
    "plots_path = Path('../data/plots')\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "X_path = datasets_path / f'X_df_BP{n_test}.csv'\n",
    "y_path = datasets_path / f'y_df_BP{n_test}.csv'\n",
    "X_test_path = datasets_path / f'X_test_df_BP{n_test}.csv'\n",
    "y_test_path = datasets_path / f'y_test_df_BP{n_test}.csv'\n",
    "\n",
    "\n",
    "if X_path.exists() and y_path.exists() and X_test_path.exists() and y_test_path.exists():\n",
    "\n",
    "    columns_to_convert = ['embeddings', 'reduced_embeddings']\n",
    "    \n",
    "    # Training data\n",
    "    X_df_BP = pd.read_csv(X_path)\n",
    "    \n",
    "    y_df_BP = pd.read_csv(y_path)\n",
    "    y_df_BP = y_df_BP.squeeze()\n",
    "    y_df_BP = y_df_BP.apply(ast.literal_eval)\n",
    "\n",
    "    \n",
    "    # Test data\n",
    "    X_test_df_BP = pd.read_csv(X_test_path)\n",
    "    \n",
    "    y_test_df_BP = pd.read_csv(y_test_path)\n",
    "    y_df_BP = y_df_BP.squeeze()\n",
    "    y_df_BP = y_df_BP.apply(ast.literal_eval)\n",
    "\n",
    "else:\n",
    "\n",
    "    X_df_BP, y_df_BP, X_test_df_BP, y_test_df_BP = load_dataset(training_data_path, test_data_path, sub_ontology)\n",
    "    X_df_BP.to_csv(X_path, index=False)\n",
    "    y_df_BP.to_csv(y_path, index=False)\n",
    "    # X_test_df_BP.to_csv(X_test_path, index=False)\n",
    "    # y_test_df_BP.to_csv(y_test_path, index=False)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "train_embeddings_path = training_data_path / 'train_embeddings.h5'\n",
    "\n",
    "train_embeddings = load_embeddings(train_embeddings_path)\n",
    "\n",
    "X_df_BP = pd.merge(train_embeddings, X_df_BP[\"ID\"], on='ID', how='right')\n",
    "\n",
    "\n",
    "X = X_df_BP['embeddings']  # Extract embeddings\n",
    "X = pd.DataFrame(X.tolist())  # Convert list of lists to DataFrame\n",
    "\n",
    "# Preprocess target (MultiLabelBinarizer for multi-label classification)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y_df_BP)\n",
    "y_tags = mlb.classes_\n",
    "\n",
    "print(f\"X shape: {X.shape} \\ny shape: {y.shape}\\n\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to PyTorch tensor\n",
    "\n",
    "# Training data\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "# Test data\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_prova = X_train_tensor[:5000]\n",
    "# y_prova = y_train_tensor[:5000]\n",
    "\n",
    "# X_prova_test = X_test_tensor[:5000]\n",
    "# y_prova_test = y_test_tensor[:5000]\n",
    "\n",
    "# X_train_tensor = X_prova\n",
    "# y_train_tensor = y_prova.to(device)\n",
    "# X_test_tensor = X_prova_test\n",
    "# y_test_tensor = y_prova_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(in_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropouts[i]))\n",
    "            in_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # For multilabel classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating/Extracting the models DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file exists and create it if it doesn't\n",
    "\n",
    "filename_df = f'NN_BP{n_test}.csv'\n",
    "nn_df_path = datasets_path / filename_df\n",
    "\n",
    "filename_results = f'NN_BP_results{n_test}.csv'\n",
    "results_path = datasets_path / filename_results\n",
    "\n",
    "filename_params = f'NN_BP_params{n_test}.csv'\n",
    "params_path = datasets_path / filename_params\n",
    "\n",
    "filename_probs = f'BP_prob{n_test}.csv'\n",
    "probs_path = datasets_path / filename_probs\n",
    "\n",
    "filename_y_pred = f'NN_BP_y_pred{n_test}.csv'\n",
    "y_pred_path = datasets_path / filename_y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if nn_df_path.exists():\n",
    "    \n",
    "    # If path exists, load the DataFrame\n",
    "    NN_BP = pd.read_csv(nn_df_path)\n",
    "    for c_list, c_int in zip(['hidden_sizes', 'dropouts'], ['batch_size', 'epochs']):\n",
    "        NN_BP[c_list] = NN_BP[c_list].apply(ast.literal_eval)\n",
    "        NN_BP[c_int] = NN_BP[c_int].astype(int)\n",
    "    \n",
    "    NN_BP['lr'] = NN_BP['lr'].astype(float)\n",
    "\n",
    "else:\n",
    "    if not datasets_path.exists():\n",
    "        datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    NN_BP = {\n",
    "        'name': [],\n",
    "        'hidden_sizes': [],\n",
    "        'dropouts': [],\n",
    "        'lr': [],\n",
    "        'batch_size': [],\n",
    "        'epochs': [],\n",
    "        'macro_P': [],\n",
    "        'macro_R': [],\n",
    "        'macro_F1': [],\n",
    "        'weighted_P': [],\n",
    "        'weighted_R': [],\n",
    "        'weighted_F1': [],\n",
    "        'samples_P': [],\n",
    "        'samples_R': [],\n",
    "        'samples_F1': [],\n",
    "        'training_time': [],\n",
    "        'probs' : []     \n",
    "    }\n",
    "    NN_BP = pd.DataFrame(NN_BP)\n",
    "\n",
    "\n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filling NN_BP with model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_parameters(df, names, hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "    for name, hidden_sizes, dropouts, lr, batch_size, epochs in zip(names, hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "        exists = name in df['name'].values\n",
    "        \n",
    "        if not exists:\n",
    "            new_row = {\n",
    "                'name': name,\n",
    "                'hidden_sizes': [hidden_sizes],\n",
    "                'dropouts': [dropouts],\n",
    "                'lr': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs,\n",
    "            }\n",
    "            \n",
    "            new_row_df = pd.DataFrame(new_row)\n",
    "            \n",
    "            NN_BP['epochs'] = NN_BP['epochs'].astype(int)\n",
    "            NN_BP['batch_size'] = NN_BP['batch_size'].astype(int)\n",
    "\n",
    "            df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{name} already exists in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(df, hidden_sizes, dropouts, lrs, batch_sizes, epochs):\n",
    "    for hidden_size in hidden_sizes:\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_row = {\n",
    "             'hidden_sizes': [hidden_size],\n",
    "             'dropouts': [dropout],\n",
    "             'lr': lr,\n",
    "             'batch_size': int(batch_size),\n",
    "             'epochs': int(epoch),\n",
    "             }\n",
    "        new_row_df = pd.DataFrame(new_row)\n",
    "        \n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "         \n",
    "\n",
    "\n",
    "def insert_parameters1(df, hidden_sizes_list, dropouts_list, lr_list, batch_sizes_list, epochs_list):\n",
    "    \n",
    "    create_paremeters(df, hidden_sizes_list, dropouts_list, lr_list, batch_sizes_list, epochs_list)\n",
    "    \n",
    "    \n",
    "    for hidden_sizes, dropouts, lr, batch_size, epochs in zip(hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "        exists = name in df['name'].values\n",
    "        \n",
    "        if not exists:\n",
    "            new_row = {\n",
    "                'hidden_sizes': [hidden_sizes],\n",
    "                'dropouts': [dropouts],\n",
    "                'lr': lr,\n",
    "                'batch_size': int(batch_size),\n",
    "                'epochs': int(epochs),\n",
    "            }\n",
    "            new_row_df = pd.DataFrame(new_row)\n",
    "            df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{name} already exists in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_sizes = [[4096, 2048],   #NN1\n",
    "#                 [4096, 2048],   #NN2\n",
    "#                 [4096, 2048],   #NN3\n",
    "#                 [2048, 1024],   #NN4\n",
    "#                 [2048, 1024],   #NN5\n",
    "#                 [2048, 1024],   #NN6\n",
    "#                 [4096, 2048, 1024],   #NN7\n",
    "#                 [4096, 2048, 2048],   #NN8\n",
    "#                 [2048, 2048, 2048],   #NN9\n",
    "#                 [2048, 4096, 2048],   #NN10\n",
    "#                 [4096, 4096, 2048],   #NN11        \n",
    "#                 [4096, 4096, 2048]]   #NN12    \n",
    "\n",
    "\n",
    "# dropouts = [0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "# lrs = [0.1, 0.01, 0.001, 0.03, 0.003, 0.05, 0.005]\n",
    "\n",
    "\n",
    "# batch_sizes = [512, 1024, 2048]\n",
    "\n",
    "# epochs = [75, 100, 150]\n",
    "\n",
    "names = ['NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6', 'NN7', 'NN8', 'NN9', 'NN10']\n",
    "\n",
    "\n",
    "hidden_sizes = [[4096, 2048],   #NN1\n",
    "                [4096, 2048],   #NN2\n",
    "                [4096, 2048],   #NN3\n",
    "                [2048, 1024],   #NN4\n",
    "                [2048, 1024],   #NN5\n",
    "                [2048, 1024],   #NN6\n",
    "                [4096, 2048, 1024],   #NN7\n",
    "                [4096, 2048, 2048],   #NN8\n",
    "                [2048, 2048, 2048],   #NN9\n",
    "                [2048, 4096, 2048],   #NN10\n",
    "                [4096, 4096, 2048],   #NN11        \n",
    "                [4096, 4096, 2048]]   #NN12    \n",
    "\n",
    "\n",
    "dropouts = [[0.1, 0.1],   #NN1 \n",
    "            [0.2, 0.2],   #NN2\n",
    "            [0.4, 0.4],   #NN3\n",
    "            [0.1, 0.1],   #NN4\n",
    "            [0.2, 0.2],   #NN5\n",
    "            [0.4, 0.4],   #NN6\n",
    "            [0.1, 0.2, 0.3],  #NN7\n",
    "            [0.2, 0.2, 0.2],  #NN8\n",
    "            [0.2, 0.2, 0.2],  #NN9\n",
    "            [0.3, 0.3, 0.3],  #NN10\n",
    "            [0.1, 0.2, 0.3],  #NN11        \n",
    "            [0.1, 0.2, 0.3]]  #NN12 \n",
    "\n",
    "\n",
    "lrs = [0.001, #NN1 \n",
    "       0.003, #NN2\n",
    "       0.005, #NN3\n",
    "       0.001, #NN4\n",
    "       0.003, #NN5\n",
    "       0.005, #NN6\n",
    "       0.005, #NN7\n",
    "       0.003, #NN8\n",
    "       0.003, #NN9\n",
    "       0.005, #NN10\n",
    "       0.003, #NN11        \n",
    "       0.001] #NN12 \n",
    "\n",
    "\n",
    "batch_sizes = [1024, #NN1\n",
    "               2048, #NN2\n",
    "               1024, #NN3\n",
    "               2048, #NN4\n",
    "               1024, #NN5\n",
    "               1024, #NN6\n",
    "               2048, #NN7\n",
    "               1024, #NN8\n",
    "               1024, #NN9\n",
    "               2048, #NN10\n",
    "               2048, #N11        \n",
    "               1024] #N12 \n",
    "\n",
    "\n",
    "epochs = [150,  #NN1 \n",
    "          75,  #NN2\n",
    "          100, #NN3\n",
    "          75,  #NN4\n",
    "          75,  #NN5\n",
    "          100, #NN6\n",
    "          75,  #NN7\n",
    "          75,  #NN8\n",
    "          75,  #NN9\n",
    "          75,  #NN10\n",
    "          75,  #NN11\n",
    "          75]  #NN12\n",
    "\n",
    "# epochs = [15,  #NN1 \n",
    "#           15,  #NN2\n",
    "#           15]#, #NN3\n",
    "#        #    25,  #NN4\n",
    "#        #    25,  #NN5\n",
    "#        #    25, #NN6\n",
    "#        #    25,  #NN7\n",
    "#        #    25,  #NN8\n",
    "#        #    25,  #NN9\n",
    "#        #    25,  #NN10\n",
    "#        #    25,  #NN11\n",
    "#        #    25]  #NN12\n",
    "\n",
    "\n",
    "\n",
    "NN_BP = insert_parameters(df = NN_BP,\n",
    "                          names=names, \n",
    "                          hidden_sizes_list=hidden_sizes, \n",
    "                          dropouts_list=dropouts, \n",
    "                          lrs=lrs, \n",
    "                          batch_sizes=batch_sizes, \n",
    "                          epochs_list=epochs)\n",
    "\n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df, name, input_size, output_size):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = NN(\n",
    "        input_size = input_size,\n",
    "        hidden_sizes = df[df['name'] == name]['hidden_sizes'].values[0],\n",
    "        dropouts = df[df['name'] == name]['dropouts'].values[0], \n",
    "        output_size = output_size\n",
    "        ).to(device)\n",
    "    \n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel\n",
    "    optimizer = optim.Adam(model.parameters(), lr=df[df['name'] == name]['lr'].values[0])\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name, X_train, y_train, optimizer, criterion, batch_size, epochs):\n",
    "    # Determine the device (model should already be on this device)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(f\"\\nTraining {name} model:\")\n",
    "\n",
    "    # Move training data to the device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            # Get the current batch\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        t = time.time() - start_time\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f} - time: {(t/60):.2f}min\")\n",
    "        elif epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f} - time: {(t/60):.2f}min\")\n",
    "\n",
    "    end_time = time.time()  # Record the end time\n",
    "\n",
    "    training_time = end_time - start_time  # Calculate the elapsed time\n",
    "\n",
    "\n",
    "    if not models_path.exists():\n",
    "        models_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_path = models_path / f'{name}_BP{n_test}.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    return training_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(df, name, model, X, y, target_names, training_time):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        probs = torch.sigmoid(y_pred)  # Assuming multilabel classification with sigmoid activation\n",
    "        \n",
    "        y_pred = (y_pred.cpu().numpy() > 0.5).astype(int)\n",
    "\n",
    "    exists = any(df[df['name'] == name])\n",
    "    \n",
    "    if exists:\n",
    "        \n",
    "        report = classification_report(y.cpu().numpy(), y_pred, target_names=target_names, output_dict=True)\n",
    "        # Convert the classification report to a DataFrame\n",
    "        report = pd.DataFrame(report).transpose().drop(columns=['support'])\n",
    "        \n",
    "        report.loc['macro avg'] = report.loc['macro avg'].apply(lambda x: round(x, 3))\n",
    "        report.loc['weighted avg'] = report.loc['weighted avg'].apply(lambda x: round(x, 3))\n",
    "        report.loc['samples avg'] = report.loc['samples avg'].apply(lambda x: round(x, 3))\n",
    "\n",
    "        # Update the DataFrame with the classification report information\n",
    "        df.loc[df['name'] == name, 'macro_P'] = report.loc['macro avg']['precision']\n",
    "        df.loc[df['name'] == name, 'macro_R'] = report.loc['macro avg']['recall']\n",
    "        df.loc[df['name'] == name, 'macro_F1'] = report.loc['macro avg']['f1-score']\n",
    "\n",
    "        df.loc[df['name'] == name, 'weighted_P'] = report.loc['weighted avg']['precision']\n",
    "        df.loc[df['name'] == name, 'weighted_R'] = report.loc['weighted avg']['recall']\n",
    "        df.loc[df['name'] == name, 'weighted_F1'] = report.loc['weighted avg']['f1-score']\n",
    "        \n",
    "        df.loc[df['name'] == name, 'samples_P'] = report.loc['samples avg']['precision']\n",
    "        df.loc[df['name'] == name, 'samples_R'] = report.loc['samples avg']['recall']\n",
    "        df.loc[df['name'] == name, 'samples_F1'] = report.loc['samples avg']['f1-score']\n",
    "\n",
    "        df.loc[df['name'] == name, 'training_time'] = training_time\n",
    "\n",
    "\n",
    "        return df, y_pred, probs\n",
    "    else:\n",
    "        print(f\"{name} already exists in the DataFrame.\")\n",
    "        return df, None, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [str(cls) for cls in mlb.classes_]\n",
    "\n",
    "\n",
    "if probs_path.exists():\n",
    "    probs = pd.read_csv(probs_path)\n",
    "    y_pred = pd.read_csv(y_pred_path)\n",
    "else:\n",
    "    probs = {}\n",
    "    y_pred = {}\n",
    "\n",
    "\n",
    "for name in NN_BP['name']:\n",
    "\n",
    "    # Create the model\n",
    "    model, optimizer, criterion = create_model(NN_BP, name, X_train_tensor.shape[1], y_train_tensor.shape[1])\n",
    "    \n",
    "\n",
    "    batch_size = int(NN_BP[NN_BP['name'] == name]['batch_size'].values[0])\n",
    "    epochs = int(NN_BP[NN_BP['name'] == name]['epochs'].values[0])\n",
    "\n",
    "    # If there is no training time, train the model\n",
    "    if np.isnan(NN_BP[NN_BP['name'] == name]['training_time'].values[0]):\n",
    "\n",
    "        filename_model = f'{name}_BP.pth'\n",
    "        model_path = models_path / filename_model\n",
    "\n",
    "        # Train the model\n",
    "        training_time = train_model(model, name, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size, epochs)\n",
    "\n",
    "        # Ensure X_test_tensor is on the same device as the model\n",
    "        device = next(model.parameters()).device\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "\n",
    "        # Evaluation\n",
    "        NN_BP, y_pred[name], probs[name] = model_evaluation(NN_BP, name, model, X_test_tensor, y_test_tensor, y_tags, training_time)\n",
    "\n",
    "        # Save the model\n",
    "        if not models_path.exists():\n",
    "            models_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    NN_BP.to_csv(nn_df_path, index=False)\n",
    "    \n",
    "\n",
    "# probs = pd.DataFrame(probs)\n",
    "      \n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['weighted_F1', 'weighted_P', 'weighted_R', \n",
    "           'macro_F1', 'macro_P', 'macro_R']\n",
    "\n",
    "\n",
    "\n",
    "NN_BP_results = NN_BP.sort_values(by=metrics, ascending=False)[['weighted_F1', 'weighted_P', 'weighted_R', \n",
    "           'macro_F1', 'macro_P', 'macro_R']][:5]\n",
    "\n",
    "NN_BP_params = NN_BP.sort_values(by=metrics, ascending=False)[['name', 'hidden_sizes', 'dropouts', 'lr', \n",
    "                                                'batch_size', 'epochs', 'training_time']][:5]\n",
    "\n",
    "NN_BP_params.head(), NN_BP_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_names = [f'NN{i}' for i in range(1, NN_BP_results.shape[0] + 1)]\n",
    "\n",
    "# Add the row means to the DataFrame\n",
    "NN_BP_results['name'] = row_names\n",
    "NN_BP_params['name'] = row_names\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "NN_BP_results.to_csv(results_path, index=False)\n",
    "NN_BP_params.to_csv(params_path, index=False)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "NN_BP_results.head(), NN_BP_params.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves(df, X_test, y_test):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for name in df['name']:\n",
    "\n",
    "        # Load the model\n",
    "        filename_model = f'{name}_BP.pth'\n",
    "        model_path = models_path / filename_model\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test)\n",
    "            y_pred_prob = y_pred.cpu().numpy()\n",
    "        \n",
    "        # Compute micro-average precision-recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test.ravel(), y_pred_prob.ravel())\n",
    "        avg_precision = average_precision_score(y_test, y_pred_prob, average=\"weighted\")\n",
    "        \n",
    "        plt.plot(recall, precision, label=f'{name} (AP={avg_precision:.4f})')\n",
    "    \n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve, model: {name}\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "\n",
    "\n",
    "    filename_plot = f'NN_PC_curve_BP.png'\n",
    "    plot_path = plots_path / filename_plot\n",
    "\n",
    "\n",
    "    if not plots_path.exists():\n",
    "        plots_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plot_precision_recall_curves(NN_BP, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_probabilities(y_pred, probs, ids, y_tags):\n",
    "    new_rows = {\n",
    "        'ID': [],\n",
    "        'GO_term': [],\n",
    "        'probabilities': []\n",
    "    }\n",
    "    \n",
    "    for i, y_p in enumerate(y_pred):\n",
    "        pos = np.where(y_p == 1)[0]\n",
    "        \n",
    "        for p in pos:\n",
    "            new_rows['ID'].append(ids[i])\n",
    "            new_rows['GO_term'].append(y_tags[p])\n",
    "            new_rows['probabilities'].append(float(probs[i][p]))\n",
    "    \n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    top_500_df = new_rows_df.sort_values(by='probabilities', ascending=False).head(500)\n",
    "    \n",
    "    return top_500_df\n",
    "\n",
    "indexes = X_test.index.to_list()    # Indexes of the test data\n",
    "ids = X_df_BP.loc[indexes, 'ID'].to_list()  # Protein IDs\n",
    "\n",
    "for name in NN_BP['name']:\n",
    "    filename_submission = f'{name}_BP_submission{n_test}.csv'\n",
    "    submission_path = datasets_path / filename_submission\n",
    "\n",
    "    p = show_probabilities(y_pred[name], probs[name], ids, y_tags)\n",
    "\n",
    "    p_sorted = p.groupby('ID', group_keys=False).apply(lambda x: x.sort_values(by='probabilities', ascending=False, ignore_index=True))\n",
    "    p_sorted.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LaTex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(df):\n",
    "    metrics = ['macro_P', 'macro_R', 'macro_F1', 'weighted_P', 'weighted_R', 'weighted_F1', 'samples_P', 'samples_R', 'samples_F1']\n",
    "    metric_names = ['Macro P', 'Macro R', 'Macro F1', 'Weighted P', 'Weighted R', 'Weighted F1', 'Samples P', 'Samples R', 'Samples F1']\n",
    "    \n",
    "    # Initialize the LaTeX table\n",
    "    latex_table = \"\\\\begin{tabular}{l\" + \"c\" * len(df['name']) + \"}\\n\"\n",
    "    latex_table += \"    \\\\toprule\\n\"\n",
    "    latex_table += \"    \\\\textit{Metric} & \" + \" & \".join([f\"\\\\textit{{{name}}}\" for name in df['name']]) + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"    \\\\midrule\\n\"\n",
    "    \n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        values = df[metric]\n",
    "        formatted_values = [f\"{value:.2f}\" for value in values]\n",
    "        latex_table += f\"    {metric_name} & \" + \" & \".join(formatted_values) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"    \\\\bottomrule\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\"\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "def generate_parameters_latex_table(df):\n",
    "    # Initialize the LaTeX table\n",
    "    latex_table = \"\\\\begin{table}[H]\\n\"\n",
    "    latex_table += \"    \\\\centering\\n\"\n",
    "    latex_table += \"    \\\\begin{tabular}{lcccccc}\\n\"\n",
    "    latex_table += \"        \\\\toprule\\n\"\n",
    "    latex_table += \"        \\\\textit{Model} & \\\\textit{Hidden Layer Sizes} & \\\\textit{Dropout} & \\\\textit{Learning Rate} & \\\\textit{Batch Size} & \\\\textit{Epochs} & \\\\textit{Regularization} \\\\\\\\\\n\"\n",
    "    latex_table += \"        \\\\midrule\\n\"\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        model_name = row['name']\n",
    "        hidden_sizes = row['hidden_sizes']\n",
    "        dropouts = row['dropouts']\n",
    "        lr = row['lr']\n",
    "        batch_size = row['batch_size']\n",
    "        epochs = row['epochs']\n",
    "        regularization = 'None'  # Assuming regularization is not provided in the DataFrame\n",
    "        \n",
    "        latex_table += f\"        {model_name} & {hidden_sizes} & {dropouts} & {lr} & {batch_size} & {epochs} & {regularization} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"        \\\\bottomrule\\n\"\n",
    "    latex_table += \"    \\\\end{tabular}\\n\"\n",
    "    latex_table += \"    \\\\caption{Model Parameters.}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "# Generate the LaTeX table\n",
    "latex_table = generate_latex_table(NN_BP_results)\n",
    "print(latex_table)\n",
    "\n",
    "latex_table = generate_parameters_latex_table(NN_BP_params)\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
