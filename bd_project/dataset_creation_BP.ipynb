{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio import Align\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import substitution_matrices\n",
    "from Bio.Data import IUPACData\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "\n",
    "'''import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval\n",
    "from cafaeval.parser import obo_parser, gt_parser'''\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the path to the main folder we are going to use to create our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to train data\n",
    "training_data_path = Path('../data/train')\n",
    "\n",
    "# Path to test data\n",
    "test_data_path = Path('../data/test')\n",
    "\n",
    "# Path to baseline data\n",
    "baseline_data_path = Path('../data/baseline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set\n",
    "\n",
    "In this section we are **creating the Training Set**, fusing togheter the data from:\n",
    "- _train_set.tsv_\n",
    "- _train_ids.txt_\n",
    "- _train.fasta_\n",
    "- _train_embeddings.h5_\n",
    "- _train_protein2ipr.dat_\n",
    "- _go-basic.obo_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `train_set.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(training_data_path / 'train_set.tsv', sep='\\t')\n",
    "\n",
    "# Rename Protein_ID and aspect columns\n",
    "train_set.rename(columns={'Protein_ID': 'ID', 'aspect' : 'sub_ontology'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the train set\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `train_ids.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting train_ids.txt\n",
    "with open(training_data_path / 'train_ids.txt', 'r') as file:\n",
    "    train_ids = file.read().splitlines()\n",
    "\n",
    "# Display the first few IDs to verify\n",
    "print(train_ids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_ids) == len(train_set['ID'].unique()):\n",
    "    print(f\"The number of IDs in train_ids.txt is equal to the number of unique IDs in the train set ({len(train_ids)}).\\n\"\n",
    "          \"Proceeding with the analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `train.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta_list = list(SeqIO.parse(training_data_path / 'train.fasta', 'fasta'))\n",
    "\n",
    "# Print the first sequence to verify\n",
    "print(train_fasta_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from SeqRecord\n",
    "train_fasta_dict = [{\n",
    "    'ID': record.id,\n",
    "    'name': record.name,\n",
    "    'description': record.description,\n",
    "    'num_features': len(record.features),\n",
    "    'sequence': record.seq,\n",
    "} for record in train_fasta_list]\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "train_fasta = pd.DataFrame(train_fasta_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "train_fasta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if `ID`, `name` and `description` have the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for differences between the ID and name columns\n",
    "diff_id_name = sum(train_fasta['ID'] != train_fasta['name'])\n",
    "\n",
    "# Checking for differences between the ID and description columns\n",
    "diff_id_description = sum(train_fasta['ID'] != train_fasta['description'])\n",
    "\n",
    "print(f\"We have a total of {diff_id_name} differences between the ID and name columns.\\nWe have a total of {diff_id_description} differences between the ID and description columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if `num_features` has value different from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_values = sum(train_fasta['num_features'] != 0)\n",
    "\n",
    "print(f\"We have a total of {num_features_values} sequences with features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice!**\n",
    "\n",
    "We found that `ID`, `name` and `description` columns are the same. We can remove two of them (`name`, `descritpion`). \n",
    "\n",
    "In the same way we saw that `num_features` is not very informative since it has only 0, let's remove it as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta.drop(columns=['name', 'description', 'num_features'], inplace=True)\n",
    "\n",
    "\n",
    "train_fasta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `train_embeddings.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"train_embeddings.h5\"\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with h5py.File(training_data_path / filename, \"r\") as f:\n",
    "    for dataset_name in f.keys():\n",
    "        dataset = f[dataset_name][:]\n",
    "        data_list.append([dataset_name, dataset])\n",
    "\n",
    "train_embeddings = pd.DataFrame(data_list, columns=[\"ID\", \"embeddings\"])\n",
    "\n",
    "train_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `train_protein2ipr.dat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protein2ipr = pd.read_csv(training_data_path / 'train_protein2ipr.dat', sep='\\t')\n",
    "\n",
    "# Rename Protein_ID and aspect columns\n",
    "train_protein2ipr.columns = ['ID', 'ipr', 'domain', 'familyID', 'start', 'end']\n",
    "\n",
    "# Display the first few rows of the train set\n",
    "train_protein2ipr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'ID' and aggregate other columns into lists\n",
    "train_protein2ipr_grouped = train_protein2ipr.groupby('ID').agg(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "print(f\"Train protein2ipr ({train_protein2ipr.shape}):\")\n",
    "train_protein2ipr_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still have to understand what *tizio, caio, sempronio* are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `go-basic.obo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = \"../data/train/go-basic.obo\"  # Replace with your file path\n",
    "\n",
    "# Step 1: Initialize storage for GO terms\n",
    "go_terms = []\n",
    "\n",
    "# Step 2: Parse the .obo file\n",
    "with open(file_path, 'r') as file:\n",
    "    current_term = {}\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Start of a new term\n",
    "        if line == \"[Term]\":\n",
    "            if current_term:  # Save the previous term\n",
    "                go_terms.append(current_term)\n",
    "            current_term = {}  # Start a new term\n",
    "            \n",
    "        elif line.startswith(\"id:\"):\n",
    "            current_term['ID'] = line.split(\"id: \")[1]\n",
    "            \n",
    "        elif line.startswith(\"alt_id:\"):\n",
    "            alt_id = line.split(\"alt_id: \")[1]\n",
    "            current_term.setdefault('alt_ids', []).append(alt_id)\n",
    "            \n",
    "        elif line.startswith(\"name:\"):\n",
    "            current_term['name'] = line.split(\"name: \")[1]\n",
    "            \n",
    "        elif line.startswith(\"namespace:\"):\n",
    "            current_term['namespace'] = line.split(\"namespace: \")[1]\n",
    "            \n",
    "        elif line.startswith(\"is_a\"):\n",
    "            match = re.search(r\"GO:\\d+\", line)  # Search for GO ID\n",
    "            if match:  # Check if a match was found\n",
    "                is_a_id = match.group()\n",
    "                current_term.setdefault('is_a', []).append(is_a_id)\n",
    "                \n",
    "        elif line.startswith(\"relationship: part_of\"):\n",
    "            match = re.search(r\"GO:\\d+\", line)  # Search for GO ID\n",
    "            if match:  # Check if a match was found\n",
    "                part_of_id = match.group()\n",
    "                current_term.setdefault('part_of', []).append(part_of_id)\n",
    "\n",
    "            \n",
    "    # Add the last term\n",
    "    if current_term:\n",
    "        go_terms.append(current_term)\n",
    "\n",
    "# Step 3: Create a unified list of all IDs (primary and alt_ids)\n",
    "expanded_terms = []\n",
    "for term in go_terms:\n",
    "    primary_id = term['ID']\n",
    "    alt_ids = term.get('alt_ids', [])\n",
    "    all_ids = [primary_id] + alt_ids\n",
    "    \n",
    "    for term_id in all_ids:\n",
    "        expanded_terms.append({\n",
    "            'ID': term_id,\n",
    "            'name': term.get('name'),\n",
    "            'namespace': term.get('namespace'),\n",
    "            'is_a': term.get('is_a', []),\n",
    "            'part_of': term.get('part_of', [])\n",
    "        })\n",
    "\n",
    "# Step 4: Convert to a DataFrame\n",
    "df = pd.DataFrame(expanded_terms)\n",
    "\n",
    "# Step 5: Filter by namespace and explode relationships\n",
    "df_is_a = df.explode('is_a').dropna(subset=['is_a'])\n",
    "df_part_of = df.explode('part_of').dropna(subset=['part_of'])\n",
    "\n",
    "\n",
    "print(\"All IDs with namespaces:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'is_a' relationships:\")\n",
    "df_is_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'part_of' relationships:\")\n",
    "df_part_of.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all previous extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us collect the dataframes and check lengths. Noting that:\n",
    "- _train_set.tsv_ contains the proteins, their GO annotations and their corresponding aspects which will be used to subdivde it into three separate datasets\n",
    "- _train.fasta_ and _train_embeddings.h5_ can be grouped together since they refers to the whole input\n",
    "- _train_protein2ipr.dat_\n",
    "- _go-basic.obo_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_combine(df, sub_ontology_value):\n",
    "    return df[df['sub_ontology'] == sub_ontology_value].groupby('ID')['GO_term'].apply(tuple).reset_index()\n",
    "\n",
    "# Create three dataframes for each sub_ontology value\n",
    "df_CC = group_and_combine(train_set, 'cellular_component')\n",
    "df_MF = group_and_combine(train_set, 'molecular_function')\n",
    "df_BP = group_and_combine(train_set, 'biological_process')\n",
    "\n",
    "print(f\"Shapes: CC {df_CC.shape}, MF {df_MF.shape}, BP {df_BP.shape}\")\n",
    "\n",
    "print(f\"Cellular Component ({df_CC.shape[0]}):\")\n",
    "df_CC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.merge(train_embeddings, train_fasta, on='ID')\n",
    "combined_train = pd.merge(combined_train, train_protein2ipr_grouped, on='ID', how='left')\n",
    "\n",
    "missing_rows = combined_train[combined_train['ipr'].isna()].shape[0]\n",
    "print(f\"Number of rows missing from train_protein2ipr_grouped: {missing_rows}\")\n",
    "\n",
    "# combined_train = combined_train.drop('domain', axis=1)\n",
    "\n",
    "print(f\"Combined DataFrame shape: {combined_train.shape}\")\n",
    "combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge combined_train with df_ab\n",
    "df_CC_full= pd.merge(combined_train, df_CC, on='ID', how='right')\n",
    "df_MF_full= pd.merge(combined_train, df_MF, on='ID', how='right')\n",
    "df_BP_full= pd.merge(combined_train, df_BP, on='ID', how='right')\n",
    "\n",
    "print(f\"Shapes: CC {df_CC_full.shape}, MF {df_MF_full.shape}, BP {df_BP_full.shape}\")\n",
    "\n",
    "print(f\"Full df CC {df_CC_full.shape}:\")\n",
    "df_CC_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df_CC = df_CC_full.iloc[:, :-1]\n",
    "y_df_BP = df_CC_full.iloc[:, -1]\n",
    "\n",
    "x_df_MF = df_MF_full.iloc[:, :-1]\n",
    "y_df_MF = df_MF_full.iloc[:, -1]\n",
    "\n",
    "X_df_BP = df_BP_full.iloc[:, :-1]\n",
    "y_df_BP = df_BP_full.iloc[:, -1]\n",
    "\n",
    "print(f\"Shapes: x_df_CC {x_df_CC.shape}, y_df_CC {y_df_BP.shape}, x_df_MF {x_df_MF.shape}, y_df_MF {y_df_MF.shape}, x_df_BP {X_df_BP.shape}, y_df_BP {y_df_BP.shape}\")\n",
    "\n",
    "print(\"CC Input:\")\n",
    "x_df_CC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CC Target:\")\n",
    "y_df_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set\n",
    "\n",
    "In this section we are **creating the Test set**, fusing togheter the data from:\n",
    "- _test_ids.txt_\n",
    "- _test.fasta_\n",
    "- _test_embeddings.h5_\n",
    "- _test_protein2ipr.dat_\n",
    "- _blast_test_results.tsv_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `test_ids.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting test_ids.txt\n",
    "with open(test_data_path / 'test_ids.txt', 'r') as file:\n",
    "    test_ids = file.read().splitlines()\n",
    "\n",
    "# Display the first few IDs to verify\n",
    "print(test_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `test.fasta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasta_list = list(SeqIO.parse(test_data_path / 'test.fasta', 'fasta'))\n",
    "\n",
    "# Print the first sequence to verify\n",
    "print(test_fasta_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from SeqRecord\n",
    "test_fasta_dict = [{\n",
    "    'ID': record.id,\n",
    "    'name': record.name,\n",
    "    'description': record.description,\n",
    "    'num_features': len(record.features),\n",
    "    'sequence': record.seq,\n",
    "} for record in test_fasta_list]\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "test_fasta = pd.DataFrame(test_fasta_dict)\n",
    "\n",
    "# Display the DataFrame\n",
    "test_fasta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if ID, name and description are the same thing, as well as seeing if num_features has relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for differences between the ID and name columns\n",
    "diff_id_name = sum(test_fasta['ID'] != test_fasta['name'])\n",
    "\n",
    "# Checking for differences between the ID and description columns\n",
    "diff_id_description = sum(test_fasta['ID'] != test_fasta['description'])\n",
    "\n",
    "print(f\"We have a total of {diff_id_name} differences between the ID and name columns.\\nWe have a total of {diff_id_description} differences between the ID and description columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_values = sum(test_fasta['num_features'] != 0)\n",
    "\n",
    "print(f\"We have a total of {num_features_values} sequences with features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the useless columns, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fasta.drop(columns=['name', 'description', 'num_features'], inplace=True)\n",
    "\n",
    "\n",
    "test_fasta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check if the IDs are the same as the ones in the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ID = len(test_fasta['ID'].unique()) # assigned because gave problem on else statement print\n",
    "\n",
    "if len(test_ids) == len_ID:\n",
    "    print(f\"The number of IDs in train_ids.txt is equal to the number of unique IDs in the train set ({len(test_ids)}).\\n\"\n",
    "          \"Proceeding with the analysis.\")\n",
    "else:\n",
    "    print(f'The numbers are not the same: test_ids are {len(test_ids)}, while the length of the fasta file is {len_ID})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `test_embeddings.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "with h5py.File(test_data_path / \"test_embeddings.h5\", \"r\") as f:\n",
    "    for dataset_name in f.keys():\n",
    "        dataset = f[dataset_name][:]\n",
    "        data_list.append([dataset_name, dataset])\n",
    "\n",
    "test_embeddings = pd.DataFrame(data_list, columns=[\"ID\", \"embeddings\"])\n",
    "\n",
    "test_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting `test_protein2ipr.dat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_protein2ipr = pd.read_csv(test_data_path / 'test_protein2ipr.dat', sep='\\t')\n",
    "\n",
    "# Rename Protein_ID and aspect columns\n",
    "test_protein2ipr.columns = ['ID', 'ipr', 'domain', 'familyID', 'start', 'end']\n",
    "\n",
    "# Remove 'domain' that is useless\n",
    "test_protein2ipr.drop('domain', axis=1)\n",
    "\n",
    "test_protein2ipr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'ID' and aggregate other columns into lists\n",
    "test_protein2ipr_grouped = test_protein2ipr.groupby('ID').agg(lambda x: tuple(x)).reset_index()\n",
    "\n",
    "print(f\"Test protein2ipr ({test_protein2ipr.shape}):\")\n",
    "test_protein2ipr_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test = pd.merge(test_embeddings, test_fasta, on='ID')\n",
    "combined_test = pd.merge(combined_test, test_protein2ipr_grouped, on='ID', how='left')\n",
    "\n",
    "missing_rows = combined_test[combined_test['ipr'].isna()].shape[0]\n",
    "print(f\"Number of rows missing from train_protein2ipr_grouped: {missing_rows}\")\n",
    "\n",
    "print(f\"Combined DataFrame shape: {combined_test.shape}\")\n",
    "combined_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if combined_train and combined_test have the same columns\n",
    "print(\"Combined_train and combined_test have the same columns:\" , set(combined_train.columns) == set(combined_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see that considering each df separately doesn't change much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_col(df, name, column_name='embeddings', variance_threshold=0.90):\n",
    "    embeddings = np.array(df[column_name].tolist())\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    embeddings_standardized = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(embeddings_standardized)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    # Components to retain (in our case 90% of variance)\n",
    "    n_components = np.argmax(np.cumsum(explained_variance) >= variance_threshold) + 1\n",
    "        \n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    plt.axhline(y=variance_threshold, color='r', linestyle='--')\n",
    "    x_value = np.argmax(np.cumsum(explained_variance) >= variance_threshold) + 1\n",
    "    plt.axvline(x=x_value, color='g', linestyle='--')\n",
    "    plt.text(x_value + 10, 0.025, f'{x_value}', color='green', size='large', weight='bold')\n",
    "    plt.text(- 50, variance_threshold + 0.025, f'{variance_threshold:.2f}', color='red', size='large', weight='bold')\n",
    "\n",
    "    plt.plot(np.cumsum(explained_variance), marker='o')\n",
    "    plt.title(f'Cumulative Explained Variance by PCA Components for {name}')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings_standardized)\n",
    "    \n",
    "    # Integrate reduced embeddings into the DataFrame\n",
    "    df[f'reduced_{column_name}'] = reduced_embeddings.tolist()\n",
    "    return df\n",
    "\n",
    "# Apply PCA and add column for each dataset\n",
    "X_df_BP = pca_col(X_df_BP, 'BP')\n",
    "#x_df_CC = pca_col(x_df_CC, 'CC')\n",
    "#x_df_MF = pca_col(x_df_MF, 'MF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "datasets_path = Path('../data/datasets')\n",
    "datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "X_df_BP.to_csv(datasets_path / 'X_df_BP.csv', index=False)\n",
    "y_df_BP.to_csv(datasets_path / 'y_df_BP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattiazza/University/biological_data/bd_project/.venv/lib/python3.13/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import ast\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from BP import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (83064, 1024) \n",
      "y shape: (83064, 1487)\n",
      "\n",
      "y_df_BP: 0    (GO:0072524, GO:0006796, GO:1901361, GO:000905...\n",
      "1    (GO:0009891, GO:0019222, GO:0010556, GO:000989...\n",
      "2    (GO:0030336, GO:0050920, GO:0048523, GO:000996...\n",
      "3    (GO:1903530, GO:0051051, GO:0046883, GO:004852...\n",
      "4    (GO:0008150, GO:2000027, GO:0050789, GO:002260...\n",
      "Name: GO_term, dtype: object \n",
      "y.head: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "type(y_df_BP): <class 'pandas.core.series.Series'> \n",
      "type(y): <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sub_ontology = 'biological_process'\n",
    "\n",
    "# Load data\n",
    "training_data_path = Path('../data/train')\n",
    "test_data_path = Path('../data/test')\n",
    "baseline_data_path = Path('../data/baseline')\n",
    "datasets_path = Path('../data/datasets')\n",
    "\n",
    "n_test = '1'\n",
    "\n",
    "X_path = datasets_path / f'X_df_BP{n_test}.csv'\n",
    "y_path = datasets_path / f'y_df_BP{n_test}.csv'\n",
    "X_test_path = datasets_path / f'X_test_df_BP{n_test}.csv'\n",
    "y_test_path = datasets_path / f'y_test_df_BP{n_test}.csv'\n",
    "\n",
    "\n",
    "if X_path.exists() and y_path.exists() and X_test_path.exists() and y_test_path.exists():\n",
    "\n",
    "    columns_to_convert = ['embeddings', 'reduced_embeddings']\n",
    "    \n",
    "    X_df_BP = pd.read_csv(X_path)\n",
    "    # for column in columns_to_convert:\n",
    "    #     X_df_BP[column] = X_df_BP[column].apply(ast.literal_eval)\n",
    "    \n",
    "    y_df_BP = pd.read_csv(y_path)\n",
    "    y_df_BP = y_df_BP.squeeze()\n",
    "    y_df_BP = y_df_BP.apply(ast.literal_eval)\n",
    "\n",
    "    \n",
    "    X_test_df_BP = pd.read_csv(X_test_path)\n",
    "    y_test_df_BP = pd.read_csv(y_test_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    X_df_BP, y_df_BP, X_test_df_BP, y_test_df_BP = load_dataset(training_data_path, test_data_path, sub_ontology)\n",
    "    X_df_BP.to_csv(X_path, index=False)\n",
    "    y_df_BP.to_csv(y_path, index=False)\n",
    "    X_test_df_BP.to_csv(X_test_path, index=False)\n",
    "    y_test_df_BP.to_csv(y_test_path, index=False)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "train_embeddings_path = training_data_path / 'train_embeddings.h5'\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with h5py.File(train_embeddings_path, \"r\") as f:\n",
    "    for dataset_name in f.keys():\n",
    "        dataset = f[dataset_name][:]\n",
    "        data_list.append([dataset_name, dataset])\n",
    "\n",
    "train_embeddings = pd.DataFrame(data_list, columns=[\"ID\", \"embeddings\"])\n",
    "\n",
    "X_df_BP = pd.merge(train_embeddings, X_df_BP[\"ID\"], on='ID', how='right')\n",
    "\n",
    "\n",
    "X = X_df_BP['embeddings']  # Extract embeddings\n",
    "X = pd.DataFrame(X.tolist())  # Convert list of lists to DataFrame\n",
    "\n",
    "# Preprocess target (MultiLabelBinarizer for multi-label classification)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y_df_BP)\n",
    "\n",
    "print(f\"X shape: {X.shape} \\ny shape: {y.shape}\\n\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy array to PyTorch tensor\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prova = X_train_tensor[:5000]\n",
    "y_prova = y_train_tensor[:5000]\n",
    "\n",
    "X_prova_test = X_test_tensor[:5000]\n",
    "y_prova_test = y_test_tensor[:5000]\n",
    "\n",
    "X_train_tensor = X_prova\n",
    "y_train_tensor = y_prova.to(device)\n",
    "X_test_tensor = X_prova_test\n",
    "y_test_tensor = y_prova_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, dropouts, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            layers.append(nn.Linear(in_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropouts[i]))\n",
    "            in_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # For multilabel classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating/Extracting the models DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>hidden_sizes</th>\n",
       "      <th>dropouts</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>macro_P</th>\n",
       "      <th>macro_R</th>\n",
       "      <th>macro_F1</th>\n",
       "      <th>weighted_P</th>\n",
       "      <th>weighted_R</th>\n",
       "      <th>weighted_F1</th>\n",
       "      <th>samples_P</th>\n",
       "      <th>samples_R</th>\n",
       "      <th>samples_F1</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN1</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.1, 0.1]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.266525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN2</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.2, 0.2]</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2048</td>\n",
       "      <td>25</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.244709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN3</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.4, 0.4]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.259160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  hidden_sizes    dropouts     lr  batch_size  epochs  macro_P  macro_R  \\\n",
       "0  NN1  [4096, 2048]  [0.1, 0.1]  0.001        1024      25    0.007    0.004   \n",
       "1  NN2  [4096, 2048]  [0.2, 0.2]  0.003        2048      25    0.006    0.004   \n",
       "2  NN3  [4096, 2048]  [0.4, 0.4]  0.005        1024      25    0.032    0.009   \n",
       "\n",
       "   macro_F1  weighted_P  weighted_R  weighted_F1  samples_P  samples_R  \\\n",
       "0     0.004       0.122       0.095        0.094      0.748      0.144   \n",
       "1     0.005       0.122       0.101        0.103      0.744      0.151   \n",
       "2     0.012       0.236       0.124        0.138      0.738      0.174   \n",
       "\n",
       "   samples_F1  training_time  \n",
       "0       0.218       0.266525  \n",
       "1       0.226       0.244709  \n",
       "2       0.249       0.259160  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the file exists and create it if it doesn't\n",
    "\n",
    "filename_df = f'NN_BP{n_test}.csv'\n",
    "filename_probs = f'BP_prob{n_test}.csv'\n",
    "filename_results = f'BP_results{n_test}.csv'\n",
    "filename_params = f'BP_params{n_test}.csv'\n",
    "\n",
    "nn_df_path = datasets_path / filename_df\n",
    "\n",
    "if nn_df_path.exists():\n",
    "    \n",
    "    # If path exists, load the DataFrame\n",
    "    NN_BP = pd.read_csv(nn_df_path)\n",
    "    for c_list, c_int in zip(['hidden_sizes', 'dropouts'], ['batch_size', 'epochs']):\n",
    "        NN_BP[c_list] = NN_BP[c_list].apply(ast.literal_eval)\n",
    "        NN_BP[c_int] = NN_BP[c_int].astype(int)\n",
    "    \n",
    "    NN_BP['lr'] = NN_BP['lr'].astype(float)\n",
    "\n",
    "else:\n",
    "    if not datasets_path.exists():\n",
    "        datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    NN_BP = {\n",
    "        'name': [],\n",
    "        'hidden_sizes': [],\n",
    "        'dropouts': [],\n",
    "        'lr': [],\n",
    "        'batch_size': [],\n",
    "        'epochs': [],\n",
    "        'macro_P': [],\n",
    "        'macro_R': [],\n",
    "        'macro_F1': [],\n",
    "        'weighted_P': [],\n",
    "        'weighted_R': [],\n",
    "        'weighted_F1': [],\n",
    "        'samples_P': [],\n",
    "        'samples_R': [],\n",
    "        'samples_F1': [],\n",
    "        'training_time': [],\n",
    "    }\n",
    "    NN_BP = pd.DataFrame(NN_BP)\n",
    "\n",
    "\n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filling NN_BP with model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_parameters(df, names, hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "    for name, hidden_sizes, dropouts, lr, batch_size, epochs in zip(names, hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "        exists = name in df['name'].values\n",
    "        \n",
    "        if not exists:\n",
    "            new_row = {\n",
    "                'name': name,\n",
    "                'hidden_sizes': [hidden_sizes],\n",
    "                'dropouts': [dropouts],\n",
    "                'lr': lr,\n",
    "                'batch_size': int(batch_size),\n",
    "                'epochs': int(epochs),\n",
    "            }\n",
    "            new_row_df = pd.DataFrame(new_row)\n",
    "            df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{name} already exists in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(df, hidden_sizes, dropouts, lrs, batch_sizes, epochs):\n",
    "    for hidden_size in hidden_sizes:\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_row = {\n",
    "             'hidden_sizes': [hidden_size],\n",
    "             'dropouts': [dropout],\n",
    "             'lr': lr,\n",
    "             'batch_size': int(batch_size),\n",
    "             'epochs': int(epoch),\n",
    "             }\n",
    "        new_row_df = pd.DataFrame(new_row)\n",
    "        \n",
    "        df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "         \n",
    "\n",
    "\n",
    "def insert_parameters1(df, hidden_sizes_list, dropouts_list, lr_list, batch_sizes_list, epochs_list):\n",
    "    \n",
    "    create_paremeters(df, hidden_sizes_list, dropouts_list, lr_list, batch_sizes_list, epochs_list)\n",
    "    \n",
    "    \n",
    "    for hidden_sizes, dropouts, lr, batch_size, epochs in zip(hidden_sizes_list, dropouts_list, lrs, batch_sizes, epochs_list):\n",
    "        exists = name in df['name'].values\n",
    "        \n",
    "        if not exists:\n",
    "            new_row = {\n",
    "                'hidden_sizes': [hidden_sizes],\n",
    "                'dropouts': [dropouts],\n",
    "                'lr': lr,\n",
    "                'batch_size': int(batch_size),\n",
    "                'epochs': int(epochs),\n",
    "            }\n",
    "            new_row_df = pd.DataFrame(new_row)\n",
    "            df = pd.concat([df, new_row_df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"{name} already exists in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN1 already exists in the DataFrame.\n",
      "NN2 already exists in the DataFrame.\n",
      "NN3 already exists in the DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>hidden_sizes</th>\n",
       "      <th>dropouts</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>macro_P</th>\n",
       "      <th>macro_R</th>\n",
       "      <th>macro_F1</th>\n",
       "      <th>weighted_P</th>\n",
       "      <th>weighted_R</th>\n",
       "      <th>weighted_F1</th>\n",
       "      <th>samples_P</th>\n",
       "      <th>samples_R</th>\n",
       "      <th>samples_F1</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN1</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.1, 0.1]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.266525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN2</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.2, 0.2]</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2048</td>\n",
       "      <td>25</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.244709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN3</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.4, 0.4]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.259160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  hidden_sizes    dropouts     lr  batch_size  epochs  macro_P  macro_R  \\\n",
       "0  NN1  [4096, 2048]  [0.1, 0.1]  0.001        1024      25    0.007    0.004   \n",
       "1  NN2  [4096, 2048]  [0.2, 0.2]  0.003        2048      25    0.006    0.004   \n",
       "2  NN3  [4096, 2048]  [0.4, 0.4]  0.005        1024      25    0.032    0.009   \n",
       "\n",
       "   macro_F1  weighted_P  weighted_R  weighted_F1  samples_P  samples_R  \\\n",
       "0     0.004       0.122       0.095        0.094      0.748      0.144   \n",
       "1     0.005       0.122       0.101        0.103      0.744      0.151   \n",
       "2     0.012       0.236       0.124        0.138      0.738      0.174   \n",
       "\n",
       "   samples_F1  training_time  \n",
       "0       0.218       0.266525  \n",
       "1       0.226       0.244709  \n",
       "2       0.249       0.259160  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden_sizes = [[4096, 2048],   #NN1\n",
    "#                 [4096, 2048],   #NN2\n",
    "#                 [4096, 2048],   #NN3\n",
    "#                 [2048, 1024],   #NN4\n",
    "#                 [2048, 1024],   #NN5\n",
    "#                 [2048, 1024],   #NN6\n",
    "#                 [4096, 2048, 1024],   #NN7\n",
    "#                 [4096, 2048, 2048],   #NN8\n",
    "#                 [2048, 2048, 2048],   #NN9\n",
    "#                 [2048, 4096, 2048],   #NN10\n",
    "#                 [4096, 4096, 2048],   #NN11        \n",
    "#                 [4096, 4096, 2048]]   #NN12    \n",
    "\n",
    "\n",
    "# dropouts = [0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "# lrs = [0.1, 0.01, 0.001, 0.03, 0.003, 0.05, 0.005]\n",
    "\n",
    "\n",
    "# batch_sizes = [512, 1024, 2048]\n",
    "\n",
    "# epochs = [75, 100, 150]\n",
    "\n",
    "names = ['NN1', 'NN2', 'NN3', 'NN4', 'NN5', 'NN6', 'NN7', 'NN8', 'NN9', 'NN10']\n",
    "\n",
    "\n",
    "hidden_sizes = [[4096, 2048],   #NN1\n",
    "                [4096, 2048],   #NN2\n",
    "                [4096, 2048],   #NN3\n",
    "                [2048, 1024],   #NN4\n",
    "                [2048, 1024],   #NN5\n",
    "                [2048, 1024],   #NN6\n",
    "                [4096, 2048, 1024],   #NN7\n",
    "                [4096, 2048, 2048],   #NN8\n",
    "                [2048, 2048, 2048],   #NN9\n",
    "                [2048, 4096, 2048],   #NN10\n",
    "                [4096, 4096, 2048],   #NN11        \n",
    "                [4096, 4096, 2048]]   #NN12    \n",
    "\n",
    "\n",
    "dropouts = [[0.1, 0.1],   #NN1 \n",
    "            [0.2, 0.2],   #NN2\n",
    "            [0.4, 0.4]]#,   #NN3\n",
    "       #      [0.1, 0.1],   #NN4\n",
    "       #      [0.2, 0.2],   #NN5\n",
    "       #      [0.4, 0.4],   #NN6\n",
    "       #      [0.1, 0.2, 0.3],  #NN7\n",
    "       #      [0.2, 0.2, 0.2],  #NN8\n",
    "       #      [0.2, 0.2, 0.2],  #NN9\n",
    "       #      [0.3, 0.3, 0.3],  #NN10\n",
    "       #      [0.1, 0.2, 0.3],  #NN11        \n",
    "       #      [0.1, 0.2, 0.3]]  #NN12 \n",
    "\n",
    "\n",
    "lrs = [0.001, #NN1 \n",
    "       0.003, #NN2\n",
    "       0.005]#, #NN3\n",
    "       # 0.001, #NN4\n",
    "       # 0.003, #NN5\n",
    "       # 0.005, #NN6\n",
    "       # 0.005, #NN7\n",
    "       # 0.003, #NN8\n",
    "       # 0.003, #NN9\n",
    "       # 0.005, #NN10\n",
    "       # 0.003, #NN11        \n",
    "       # 0.001] #NN12 \n",
    "\n",
    "\n",
    "batch_sizes = [1024, #NN1\n",
    "               2048, #NN2\n",
    "               1024]#, #NN3\n",
    "              #  2048, #NN4\n",
    "              #  1024, #NN5\n",
    "              #  1024, #NN6\n",
    "              #  2048, #NN7\n",
    "              #  1024, #NN8\n",
    "              #  1024, #NN9\n",
    "              #  2048, #NN10\n",
    "              #  2048, #N11        \n",
    "              #  1024] #N12 \n",
    "\n",
    "\n",
    "# epochs = [120,  #NN1 \n",
    "#           75,  #NN2\n",
    "#           100, #NN3\n",
    "#           75,  #NN4\n",
    "#           75,  #NN5\n",
    "#           100, #NN6\n",
    "#           75,  #NN7\n",
    "#           75,  #NN8\n",
    "#           75,  #NN9\n",
    "#           75,  #NN10\n",
    "#           75,  #NN11\n",
    "#           75]  #NN12\n",
    "\n",
    "epochs = [25,  #NN1 \n",
    "          25,  #NN2\n",
    "          25]#, #NN3\n",
    "       #    25,  #NN4\n",
    "       #    25,  #NN5\n",
    "       #    25, #NN6\n",
    "       #    25,  #NN7\n",
    "       #    25,  #NN8\n",
    "       #    25,  #NN9\n",
    "       #    25,  #NN10\n",
    "       #    25,  #NN11\n",
    "       #    25]  #NN12\n",
    "\n",
    "\n",
    "\n",
    "NN_BP = insert_parameters(df = NN_BP,\n",
    "                          names=names, \n",
    "                          hidden_sizes_list=hidden_sizes, \n",
    "                          dropouts_list=dropouts, \n",
    "                          lrs=lrs, \n",
    "                          batch_sizes=batch_sizes, \n",
    "                          epochs_list=epochs)\n",
    "\n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(df, name, input_size, output_size):\n",
    "\n",
    "    hidd_s = df[df['name'] == name]['hidden_sizes'].tolist()\n",
    "    dropouts = df[df['name'] == name]['dropouts'].tolist()\n",
    "    # Initialize the model\n",
    "    model = NN(\n",
    "        input_size = input_size,\n",
    "        hidden_sizes = hidd_s,\n",
    "        dropouts = dropouts, \n",
    "        output_size = output_size\n",
    "        ).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel\n",
    "    optimizer = optim.Adam(model.parameters(), lr=df[df['name'] == name]['lr'])\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, name, X_train, y_train, optimizer, criterion, batch_size, epochs):\n",
    "    # Determine the device (model should already be on this device)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    print(f\"\\nTraining {name} model:\")\n",
    "\n",
    "    # Move training data to the device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    model.train()\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            # Get the current batch\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        t = time.time() - start_time\n",
    "\n",
    "        if epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f} - time: {(t/60):.2f}min\")\n",
    "        elif epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f} - time: {(t/60):.2f}min\")\n",
    "\n",
    "    end_time = time.time()  # Record the end time\n",
    "\n",
    "    training_time = end_time - start_time  # Calculate the elapsed time\n",
    "    \n",
    "    return training_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(df, name, model, X, y, target_names, training_time):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        probabilities = torch.sigmoid(y_pred)  # Assuming multilabel classification with sigmoid activation\n",
    "        prob = probabilities[probabilities > 0.20].float()\n",
    "        \n",
    "        y_pred = (y_pred.cpu().numpy() > 0.5).astype(int)\n",
    "\n",
    "    exists = any(df[df['name'] == name])\n",
    "    \n",
    "    if exists:\n",
    "        \n",
    "        report = classification_report(y.cpu().numpy(), y_pred, target_names=target_names, output_dict=True)\n",
    "        # Convert the classification report to a DataFrame\n",
    "        report = pd.DataFrame(report).transpose().drop(columns=['support'])\n",
    "        \n",
    "        report.loc['macro avg'] = report.loc['macro avg'].apply(lambda x: round(x, 3))\n",
    "        report.loc['weighted avg'] = report.loc['weighted avg'].apply(lambda x: round(x, 3))\n",
    "        report.loc['samples avg'] = report.loc['samples avg'].apply(lambda x: round(x, 3))\n",
    "\n",
    "        # Update the DataFrame with the classification report information\n",
    "        df.loc[df['name'] == name, 'macro_P'] = report.loc['macro avg']['precision']\n",
    "        df.loc[df['name'] == name, 'macro_R'] = report.loc['macro avg']['recall']\n",
    "        df.loc[df['name'] == name, 'macro_F1'] = report.loc['macro avg']['f1-score']\n",
    "\n",
    "        df.loc[df['name'] == name, 'weighted_P'] = report.loc['weighted avg']['precision']\n",
    "        df.loc[df['name'] == name, 'weighted_R'] = report.loc['weighted avg']['recall']\n",
    "        df.loc[df['name'] == name, 'weighted_F1'] = report.loc['weighted avg']['f1-score']\n",
    "        \n",
    "        df.loc[df['name'] == name, 'samples_P'] = report.loc['samples avg']['precision']\n",
    "        df.loc[df['name'] == name, 'samples_R'] = report.loc['samples avg']['recall']\n",
    "        df.loc[df['name'] == name, 'samples_F1'] = report.loc['samples avg']['f1-score']\n",
    "\n",
    "        df.loc[df['name'] == name, 'training_time'] = training_time\n",
    "\n",
    "        return df, prob\n",
    "    else:\n",
    "        print(f\"{name} already exists in the DataFrame.\")\n",
    "        return df, None, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>hidden_sizes</th>\n",
       "      <th>dropouts</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>macro_P</th>\n",
       "      <th>macro_R</th>\n",
       "      <th>macro_F1</th>\n",
       "      <th>weighted_P</th>\n",
       "      <th>weighted_R</th>\n",
       "      <th>weighted_F1</th>\n",
       "      <th>samples_P</th>\n",
       "      <th>samples_R</th>\n",
       "      <th>samples_F1</th>\n",
       "      <th>training_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN1</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.1, 0.1]</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.266525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN2</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.2, 0.2]</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2048</td>\n",
       "      <td>25</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.244709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN3</td>\n",
       "      <td>[4096, 2048]</td>\n",
       "      <td>[0.4, 0.4]</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1024</td>\n",
       "      <td>25</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.259160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  hidden_sizes    dropouts     lr  batch_size  epochs  macro_P  macro_R  \\\n",
       "0  NN1  [4096, 2048]  [0.1, 0.1]  0.001        1024      25    0.007    0.004   \n",
       "1  NN2  [4096, 2048]  [0.2, 0.2]  0.003        2048      25    0.006    0.004   \n",
       "2  NN3  [4096, 2048]  [0.4, 0.4]  0.005        1024      25    0.032    0.009   \n",
       "\n",
       "   macro_F1  weighted_P  weighted_R  weighted_F1  samples_P  samples_R  \\\n",
       "0     0.004       0.122       0.095        0.094      0.748      0.144   \n",
       "1     0.005       0.122       0.101        0.103      0.744      0.151   \n",
       "2     0.012       0.236       0.124        0.138      0.738      0.174   \n",
       "\n",
       "   samples_F1  training_time  \n",
       "0       0.218       0.266525  \n",
       "1       0.226       0.244709  \n",
       "2       0.249       0.259160  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = [str(cls) for cls in mlb.classes_]\n",
    "\n",
    "probs_path = datasets_path / filename_probs\n",
    "\n",
    "\n",
    "if probs_path.exists():\n",
    "    probs = pd.read_csv(datasets_path / filename_probs)\n",
    "else:\n",
    "    probs = {}\n",
    "\n",
    "for name in NN_BP['name']:\n",
    "\n",
    "    # Define model parameters\n",
    "    input_size = X_train_tensor.shape[1] \n",
    "    hidden_sizes = NN_BP[NN_BP['name'] == name]['hidden_sizes'].values[0]\n",
    "    dropouts = NN_BP[NN_BP['name'] == name]['dropouts'].values[0]\n",
    "    output_size = y_train_tensor.shape[1]\n",
    "    lr = float(NN_BP[NN_BP['name'] == name]['lr'].values[0])\n",
    "    batch_size = int(NN_BP[NN_BP['name'] == name]['batch_size'].values[0])\n",
    "    epochs = int(NN_BP[NN_BP['name'] == name]['epochs'].values[0])\n",
    "\n",
    "    # Initialize the model\n",
    "    model = NN(\n",
    "        input_size = input_size,\n",
    "        hidden_sizes = hidden_sizes,\n",
    "        dropouts = dropouts, \n",
    "        output_size = output_size\n",
    "        ).to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # model, optimizer, criterion = create_model(NN_BP, name, X_train_tensor.shape[1] , y_train_tensor.shape[1])\n",
    "    if np.isnan(NN_BP[NN_BP['name'] == name]['training_time'].values[0]):\n",
    "        # Train the model\n",
    "        training_time = train_model(model, name, X_train_tensor, y_train_tensor, optimizer, criterion, batch_size, epochs)\n",
    "\n",
    "        # Ensure X_test_tensor is on the same device as the model\n",
    "        device = next(model.parameters()).device\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "\n",
    "        # Evaluation\n",
    "        \n",
    "        NN_BP, probs[name] = model_evaluation(NN_BP, name, model, X_test_tensor, y_test_tensor, target_names, training_time)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    NN_BP.to_csv(nn_df_path, index=False)\n",
    "    \n",
    "\n",
    "# probs = pd.DataFrame(probs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "NN_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs1 = pd.DataFrame(probs)\n",
    "probs1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  name  hidden_sizes    dropouts     lr  batch_size  epochs  training_time\n",
       " 2  NN3  [4096, 2048]  [0.4, 0.4]  0.005        1024      25       0.259160\n",
       " 1  NN2  [4096, 2048]  [0.2, 0.2]  0.003        2048      25       0.244709\n",
       " 0  NN1  [4096, 2048]  [0.1, 0.1]  0.001        1024      25       0.266525,\n",
       "    weighted_F1  weighted_P  weighted_R  macro_F1  macro_P  macro_R  \\\n",
       " 2        0.138       0.236       0.124     0.012    0.032    0.009   \n",
       " 1        0.103       0.122       0.101     0.005    0.006    0.004   \n",
       " 0        0.094       0.122       0.095     0.004    0.007    0.004   \n",
       " \n",
       "    samples_F1  samples_P  samples_R  \n",
       " 2       0.249      0.738      0.174  \n",
       " 1       0.226      0.744      0.151  \n",
       " 0       0.218      0.748      0.144  )"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['weighted_F1', 'weighted_P', 'weighted_R', \n",
    "           'macro_F1', 'macro_P', 'macro_R',  \n",
    "           'samples_F1', 'samples_P', 'samples_R']\n",
    "\n",
    "\n",
    "\n",
    "BP_results = NN_BP.sort_values(by=metrics, ascending=False)[['weighted_F1', 'weighted_P', 'weighted_R', \n",
    "           'macro_F1', 'macro_P', 'macro_R',  \n",
    "           'samples_F1', 'samples_P', 'samples_R']][:5]\n",
    "\n",
    "BP_params = NN_BP.sort_values(by=metrics, ascending=False)[['name', 'hidden_sizes', 'dropouts', 'lr', \n",
    "                                                'batch_size', 'epochs', 'training_time']][:5]\n",
    "\n",
    "BP_params.head(), BP_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   weighted_F1  weighted_P  weighted_R  macro_F1  macro_P  macro_R  \\\n",
       " 2        0.138       0.236       0.124     0.012    0.032    0.009   \n",
       " 1        0.103       0.122       0.101     0.005    0.006    0.004   \n",
       " 0        0.094       0.122       0.095     0.004    0.007    0.004   \n",
       " \n",
       "    samples_F1  samples_P  samples_R name  \n",
       " 2       0.249      0.738      0.174  NN1  \n",
       " 1       0.226      0.744      0.151  NN2  \n",
       " 0       0.218      0.748      0.144  NN3  ,\n",
       "   name  hidden_sizes    dropouts     lr  batch_size  epochs  training_time\n",
       " 2  NN1  [4096, 2048]  [0.4, 0.4]  0.005        1024      25       0.259160\n",
       " 1  NN2  [4096, 2048]  [0.2, 0.2]  0.003        2048      25       0.244709\n",
       " 0  NN3  [4096, 2048]  [0.1, 0.1]  0.001        1024      25       0.266525)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_names = [f'NN{i}' for i in range(1, BP_results.shape[0] + 1)]\n",
    "\n",
    "# Add the row means to the DataFrame\n",
    "BP_results['name'] = row_names\n",
    "BP_params['name'] = row_names\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "BP_results.to_csv(datasets_path / filename_results, index=False)\n",
    "BP_params.to_csv(datasets_path / filename_params, index=False)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "BP_results.head(), BP_params.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LaTex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lccc}\n",
      "    \\toprule\n",
      "    \\textit{Metric} & \\textit{NN1} & \\textit{NN2} & \\textit{NN3} \\\\\n",
      "    \\midrule\n",
      "    Macro P & 0.03 & 0.01 & 0.01 \\\\\n",
      "    Macro R & 0.01 & 0.00 & 0.00 \\\\\n",
      "    Macro F1 & 0.01 & 0.01 & 0.00 \\\\\n",
      "    Weighted P & 0.24 & 0.12 & 0.12 \\\\\n",
      "    Weighted R & 0.12 & 0.10 & 0.10 \\\\\n",
      "    Weighted F1 & 0.14 & 0.10 & 0.09 \\\\\n",
      "    Samples P & 0.74 & 0.74 & 0.75 \\\\\n",
      "    Samples R & 0.17 & 0.15 & 0.14 \\\\\n",
      "    Samples F1 & 0.25 & 0.23 & 0.22 \\\\\n",
      "    \\bottomrule\n",
      "\\end{tabular}\n",
      "\\begin{table}[H]\n",
      "    \\centering\n",
      "    \\begin{tabular}{lcccccc}\n",
      "        \\toprule\n",
      "        \\textit{Model} & \\textit{Hidden Layer Sizes} & \\textit{Dropout} & \\textit{Learning Rate} & \\textit{Batch Size} & \\textit{Epochs} & \\textit{Regularization} \\\\\n",
      "        \\midrule\n",
      "        NN1 & [4096, 2048] & [0.4, 0.4] & 0.005 & 1024 & 25 & None \\\\\n",
      "        NN2 & [4096, 2048] & [0.2, 0.2] & 0.003 & 2048 & 25 & None \\\\\n",
      "        NN3 & [4096, 2048] & [0.1, 0.1] & 0.001 & 1024 & 25 & None \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{Model Parameters.}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "def generate_latex_table(df):\n",
    "    metrics = ['macro_P', 'macro_R', 'macro_F1', 'weighted_P', 'weighted_R', 'weighted_F1', 'samples_P', 'samples_R', 'samples_F1']\n",
    "    metric_names = ['Macro P', 'Macro R', 'Macro F1', 'Weighted P', 'Weighted R', 'Weighted F1', 'Samples P', 'Samples R', 'Samples F1']\n",
    "    \n",
    "    # Initialize the LaTeX table\n",
    "    latex_table = \"\\\\begin{tabular}{l\" + \"c\" * len(df['name']) + \"}\\n\"\n",
    "    latex_table += \"    \\\\toprule\\n\"\n",
    "    latex_table += \"    \\\\textit{Metric} & \" + \" & \".join([f\"\\\\textit{{{name}}}\" for name in df['name']]) + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"    \\\\midrule\\n\"\n",
    "    \n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        values = df[metric]\n",
    "        formatted_values = [f\"{value:.2f}\" for value in values]\n",
    "        latex_table += f\"    {metric_name} & \" + \" & \".join(formatted_values) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"    \\\\bottomrule\\n\"\n",
    "    latex_table += \"\\\\end{tabular}\"\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "\n",
    "def generate_parameters_latex_table(df):\n",
    "    # Initialize the LaTeX table\n",
    "    latex_table = \"\\\\begin{table}[H]\\n\"\n",
    "    latex_table += \"    \\\\centering\\n\"\n",
    "    latex_table += \"    \\\\begin{tabular}{lcccccc}\\n\"\n",
    "    latex_table += \"        \\\\toprule\\n\"\n",
    "    latex_table += \"        \\\\textit{Model} & \\\\textit{Hidden Layer Sizes} & \\\\textit{Dropout} & \\\\textit{Learning Rate} & \\\\textit{Batch Size} & \\\\textit{Epochs} & \\\\textit{Regularization} \\\\\\\\\\n\"\n",
    "    latex_table += \"        \\\\midrule\\n\"\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        model_name = row['name']\n",
    "        hidden_sizes = row['hidden_sizes']\n",
    "        dropouts = row['dropouts']\n",
    "        lr = row['lr']\n",
    "        batch_size = row['batch_size']\n",
    "        epochs = row['epochs']\n",
    "        regularization = 'None'  # Assuming regularization is not provided in the DataFrame\n",
    "        \n",
    "        latex_table += f\"        {model_name} & {hidden_sizes} & {dropouts} & {lr} & {batch_size} & {epochs} & {regularization} \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_table += \"        \\\\bottomrule\\n\"\n",
    "    latex_table += \"    \\\\end{tabular}\\n\"\n",
    "    latex_table += \"    \\\\caption{Model Parameters.}\\n\"\n",
    "    latex_table += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_table\n",
    "\n",
    "# Generate the LaTeX table\n",
    "latex_table = generate_latex_table(BP_results)\n",
    "print(latex_table)\n",
    "\n",
    "latex_table = generate_parameters_latex_table(BP_params)\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
